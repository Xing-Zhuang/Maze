[project]
name = "maze-agent"  # 这是pip install时使用的发布名
version = "0.1.0"
# Main Server Configuration
[server]
host = "10.18.108.1"
port = 6380 # 统一的服务端口
strategy = "daps" # 默认的DAG调度策略

# Path Configurations
[paths]
# 项目根目录, 用于 Ray 的 runtime_env。请根据您的实际情况修改。
# 注意：这个路径应该是包含 'maze' 文件夹的那个目录。
project_root = "/root/workspace/d23oa7cp420c73acue30"
# 模型缓存目录 (相对于 project_root)
model_cache_dir = "model_cache"

# Online API Configurations
[online_apis]
    # 可以定义多个不同的API服务商或模型配置
    [online_apis.default_llm]
    model = "deepseek-ai/DeepSeek-V3.1"
    base_url = "https://api.siliconflow.cn/v1/"
    api_key = "sk-atbmbuobehrzbiagahmshyrdksemgfkztnljehdnojlakwkq"
    use_online_model = true
    max_tokens = 16384
    temperature = 0.6
    top_p = 0.9
    repetition_penalty = 1.1
    text_batch_size= 8
    [online_apis.default_vlm]
    model = "Qwen/Qwen2.5-VL-72B-Instruct"
    base_url = "https://api.siliconflow.cn/v1/"
    api_key = "sk-atbmbuobehrzbiagahmshyrdksemgfkztnljehdnojlakwkq"
    max_tokens = 16384
    temperature = 0.6
    top_p = 0.9
    repetition_penalty = 1.1
    vlm_batch_size= 8

# Local Model Configurations
[local_models]
    [local_models.qwen3-32b]
    path = "Qwen/Qwen3-32B"
    served-model-name = "qwen3-32b"
    gpu_mem = 80000
    tensor-parallel-size = 1
    max-model-len = 16384
    [local_models.deepseek-r1-32b]
    path = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
    served-model-name = "deepseek-r1-32b"
    gpu_mem = 80000
    tensor-parallel-size = 1
    max-model-len = 16384
    [local_models.qwen2.5-vl-32b]
    path = "Qwen/Qwen2.5-VL-32B-Instruct"
    served-model-name = "qwen2.5-vl-32b"
    gpu_mem = 80000
    tensor-parallel-size = 1
    max-model-len = 8192