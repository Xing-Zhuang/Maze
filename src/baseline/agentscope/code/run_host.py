import base64
import os
import time
import json
import uuid
import threading
import asyncio
import argparse
import ast
from itertools import cycle
from typing import Dict, List, Any, Optional
from flask import Flask, request, jsonify
from baseline.utils.query_loader import GaiaLoader, TBenchLoader, OpenAGILoader
import traceback
import csv
from worker_agent import *
from agentscope.message import Msg
from agentscope.rpc.retry_strategy import RetryFixedTimes
from concurrent.futures import ProcessPoolExecutor
from concurrent.futures import ThreadPoolExecutor

# å°†ä»»åŠ¡é˜Ÿåˆ—å®šä¹‰åœ¨å…¨å±€ï¼Œä»¥ä¾¿Flaskå’Œåå°å¾ªç¯éƒ½èƒ½è®¿é—®
dag_queue = asyncio.Queue()

# å°†æ­¤æ˜ å°„åŒæ ·ç½®äºå…¨å±€ï¼Œä»¥ä¾¿å­è¿›ç¨‹å¯ä»¥è®¿é—®
WORKFLOW_TYPE_TO_CLASS_MAP = {
    "gaia_file": GAIA_File_Process_Agent,
    "gaia_speech": GAIA_Speech_Agent,
    "gaia_reason": GAIA_Reason_Agent,
    "gaia_vision": GAIA_Vision_Agent,
    "tbench_airline_book": tbench_airline_book_Agent,
    "tbench_airline_cancel": tbench_airline_cancel_Agent,
    "tbench_retail_cancel": tbench_retail_cancel_Agent,
    "tbench_retail_return": tbench_retail_return_Agent,
    "tbench_retail_modify": tbench_retail_modify_Agent,
    "tbench_retail_cancel_modify": tbench_retail_cancel_modify_Agent,
    "openagi_document_qa": openagi_document_qa_Agent,
    "openagi_image_captioning_complex": openagi_image_captioning_complex_Agent,
    "openagi_multimodal_vqa_complex": openagi_multimodal_vqa_complex_Agent,
    "openagi_text_processing_multilingual": openagi_text_processing_multilingual_Agent
}

# ==================== é¡¶å±‚å‡½æ•°ï¼Œç”¨äºè¿›ç¨‹æ± æ‰§è¡Œ ====================
def run_reply_in_process(workflow_type: str, agent_name: str, host: str, port: int, msg_content: str) -> str:
    """
    è¿™ä¸ªå‡½æ•°å°†åœ¨ä¸€ä¸ªå®Œå…¨ç‹¬ç«‹çš„å­è¿›ç¨‹ä¸­è¢«æ‰§è¡Œï¼Œä»è€Œéš”ç¦»æ‰€æœ‰èµ„æºã€‚
    æ³¨æ„ï¼šä¸ºäº†è·¨è¿›ç¨‹ä¼ é€’ï¼Œæˆ‘ä»¬å°†å®Œæ•´çš„Msgå¯¹è±¡ç®€åŒ–ä¸ºåªä¼ é€’å…¶contentå­—ç¬¦ä¸²ã€‚
    """
    try:
        # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°æ„å»ºMsgå¯¹è±¡
        msg = Msg(name="", role="assistant", content=msg_content)

        AgentClass = WORKFLOW_TYPE_TO_CLASS_MAP.get(workflow_type)
        if not AgentClass:
            raise Exception(f"å­è¿›ç¨‹ä¸­æœªæ‰¾åˆ°ç±»å‹ {workflow_type} å¯¹åº”çš„ Agent ç±»")

        # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°åˆ›å»ºRPCä»£ç†
        agent_proxy = AgentClass(id=agent_name, workflow_type=workflow_type).to_dist(
            host=host,
            port=port,
            retry_strategy=RetryFixedTimes(max_retries=600, delay=10)
        )
        # æ‰§è¡Œé˜»å¡çš„RPCè°ƒç”¨
        response_msg = agent_proxy.reply(msg)
        # åªè¿”å›å¯åºåˆ—åŒ–çš„ç»“æœå†…å®¹
        return response_msg.content
    except Exception as e:
        # åœ¨å­è¿›ç¨‹ä¸­æ•è·å¼‚å¸¸ï¼Œå¹¶å°†å…¶ä½œä¸ºå­—ç¬¦ä¸²è¿”å›ï¼Œä»¥ä¾¿ä¸»è¿›ç¨‹å¯ä»¥å¤„ç†
        error_info = {
            "error": f"å­è¿›ç¨‹æ‰§è¡Œå¤±è´¥: {e}",
            "traceback": traceback.format_exc()
        }
        return json.dumps(error_info)
# =================================================================================

def str_to_bool(val):
    if isinstance(val, bool): return val
    if val.lower() in ('true', 't', 'yes', '1'): return True
    elif val.lower() in ('false', 'f', 'no', '0'): return False
    else: raise argparse.ArgumentTypeError('Boolean value expected.')

def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="AgentOS Host: å¯åŠ¨ä¸»æœåŠ¡ã€‚")
    parser.add_argument("--agent_pools", type=str, required=True, help='å®šä¹‰å·¥ä½œæµç±»å‹ä¸Workeræ± çš„JSONå­—ç¬¦ä¸²ã€‚')
    parser.add_argument("--host", type=str, default="127.0.0.1", help="Flask API ç›‘å¬çš„ä¸»æœºåœ°å€ã€‚")
    parser.add_argument("--port", type=int, default=5002, help="Flask API çš„ç›‘å¬ç«¯å£ã€‚")
    parser.add_argument("--proj_path", type=str, default="/root/workspace/d23oa7cp420c73acue30/AgentOS", help="é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚")
    parser.add_argument("--model_folder", default="model_cache", help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    parser.add_argument("--data_path", type=str, default="data/", help="æ•°æ®ç›®å½•ç›¸å¯¹è·¯å¾„ã€‚")
    parser.add_argument("--dag_path", type=str, default="src/benchmarks/workflows", help="DAG å®šä¹‰ç›®å½•ç›¸å¯¹è·¯å¾„ã€‚")
    parser.add_argument("--api_url", default="https://api.siliconflow.cn/v1/chat/completions", help="åœ¨çº¿æ¨¡å‹API URL")
    parser.add_argument("--api_key", default="Bearer sk-jbkxfkvrtluiezhqcvflmvenetulbluzpshppqqqtgxzswce", help="åœ¨çº¿æ¨¡å‹API Key")
    parser.add_argument("--temperature", type=float, default=0.6, help="æ¨¡å‹é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--max_token", type=int, default=1024, help="æ¨¡å‹æœ€å¤§token")
    parser.add_argument("--top_p", type=float, default=0.9, help="æ¨¡å‹top_p")
    parser.add_argument("--repetition_penalty", type=float, default=1.1, help="æ¨¡å‹é‡å¤æƒ©ç½š")
    parser.add_argument("--use_online_model", type=str_to_bool, default=False, help="æ˜¯å¦ä½¿ç”¨åœ¨çº¿æ¨¡å‹")
    parser.add_argument("--vlm_batch_size", type=int, default=8, help="VLMæ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--text_batch_size", type=int, default=8, help="æ–‡æœ¬æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--task_exec_time_csv_path", default="src/baseline/agentscope/results/task_exec_time.csv")
    parser.add_argument("--task_exec_result_jsonl_path", default="src/baseline/agentscope/results/task_exec_result.jsonl")
    args = parser.parse_args()
    args.agent_pools = json.loads(args.agent_pools)
    args.model_folder = os.path.join(args.proj_path, args.model_folder)
    args.task_exec_time_csv_path = os.path.join(args.proj_path, args.task_exec_time_csv_path)
    args.task_exec_result_jsonl_path = os.path.join(args.proj_path, args.task_exec_result_jsonl_path)
    return args

args = parse_arguments()

class RoundRobinRouter:
    def __init__(self, agent_pools: Dict[str, List[str]]):
        self._next_agent_iterators = {
            workflow_type: cycle(agents)
            for workflow_type, agents in agent_pools.items()
        }
        print("âœ… è½®è¯¢è·¯ç”±å™¨å·²åˆå§‹åŒ–ï¼ŒWorkeræ± é…ç½®å¦‚ä¸‹:")
        for w_type, agents in agent_pools.items():
            print(f"   - ç±»å‹ '{w_type}': {agents}")

    def get_next_worker_id(self, workflow_type: str) -> Optional[str]:
        iterator = self._next_agent_iterators.get(workflow_type)
        return next(iterator) if iterator else None

class TaskLoader:
    def __init__(self, args: argparse.Namespace):
        self.args = args
        self.query_loader_factory = {"gaia": GaiaLoader, "tbench": TBenchLoader, "openagi": OpenAGILoader}

    def load_workflow_message(self, dag_id: str, dag_type: str, dag_source: str, supplementary_files: List[str], sub_time: float) -> Msg:
        loader_class = self.query_loader_factory.get(dag_source)
        loader = loader_class(args=self.args, dag_id=dag_id, dag_type=dag_type, dag_source=dag_source, supplementary_files=supplementary_files, sub_time=sub_time)
        question, answer, arrival_time = loader.question, loader.answer, loader.arrival_time
        file_contents_base64 = {}
        file_paths_map = loader.get_supplementary_files()
        if file_paths_map:
            for filename, file_path in file_paths_map.items():
                with open(file_path, 'rb') as f:
                    content_bytes = f.read()
                file_contents_base64[filename] = base64.b64encode(content_bytes).decode('utf-8')
        arg_src = {"dag_id": dag_id, "question": question, "args": json.dumps(vars(self.args)), "supplementary_files": file_contents_base64}
        task_info = {"dag_id": dag_id, "question": question, "uuid": str(uuid.uuid4()), "arrival_time": arrival_time, "sub_time": sub_time, "arg_src": arg_src, "answer": answer, "type": f"{dag_source}_{dag_type}"}
        return Msg(name="", role="assistant", content=str(task_info))

class Dispatcher:
    """æœ€ç»ˆç‰ˆè°ƒåº¦å™¨ï¼Œå®ç°äº†æœºå™¨çº§ï¼ˆåŸºäºHost IPï¼‰çš„å¹¶å‘æ§åˆ¶"""
    def __init__(self, router: "RoundRobinRouter", active_tasks: Dict, final_results: Dict, executor):
        self.router = router
        self.active_tasks = active_tasks
        self.final_results = final_results
        self.executor = executor
        self.machine_locks: Dict[str, asyncio.Lock] = {}
        self.lock_management_lock = asyncio.Lock()

    async def _execute_and_process_result(
        self, msg_content: str, uid: str, workflow_type: str, worker_id: str,
        agent_name: str, host: str, port: int, machine_lock: asyncio.Lock
    ):
        """è¿™ä¸ªæ–¹æ³•è´Ÿè´£æ‰€æœ‰è€—æ—¶æ“ä½œï¼Œå¹¶åœ¨æ‰§è¡Œå‰è·å–æœºå™¨é”"""
        print(f"â³ ä»»åŠ¡ {uid} æ­£åœ¨æ’é˜Ÿï¼Œç­‰å¾…æœºå™¨ [{host}] å˜ä¸ºç©ºé—²...")
        async with machine_lock:
            print(f"ğŸŸ¢ æœºå™¨ [{host}] å·²é”å®šï¼Œä»»åŠ¡ {uid} å¼€å§‹åœ¨å­è¿›ç¨‹ä¸­æ‰§è¡Œã€‚")
            self.active_tasks[uid].update({"status": "Running", "dispatched_to": worker_id})
            try:
                loop = asyncio.get_running_loop()
                response_content_str = await loop.run_in_executor(
                    self.executor, run_reply_in_process,
                    workflow_type, agent_name, host, port, msg_content
                )
                
                try:
                    potential_error = json.loads(response_content_str)
                    if isinstance(potential_error, dict) and "error" in potential_error:
                        raise Exception(potential_error.get('traceback', potential_error['error']))
                except (json.JSONDecodeError, TypeError):
                    pass
                
                response_content = ast.literal_eval(response_content_str)
                res_uuid = response_content.get("uuid")
                
                print(f"âœ… [Dispatcher] ä»»åŠ¡ {res_uuid} æ‰§è¡ŒæˆåŠŸã€‚")
                self.active_tasks[res_uuid].update({"status": "Finished"})
                self.final_results[res_uuid] = response_content

                # ==================== å¡«å……æ‚¨ç¼ºå¤±çš„æ–‡ä»¶å†™å…¥é€»è¾‘ ====================
                try:
                    response_content["leave_time"] = time.time()
                    with open(args.task_exec_time_csv_path, 'a', newline='') as f:
                        writer = csv.writer(f)
                        row = [
                            response_content.get("dag_id"), response_content.get("uuid"), response_content.get("sub_time"),
                            response_content.get("arrival_time"), response_content.get("start_time"), response_content.get("end_time"),
                            response_content.get("end_time", 0) - response_content.get("start_time", 0),
                            response_content.get("leave_time"),
                            response_content.get("end_time", 0) - response_content.get("arrival_time", 0),
                            response_content.get("leave_time", 0) - response_content.get("sub_time", 0)
                        ]
                        writer.writerow(row)
                    print(f"DAG {response_content.get('dag_id')} execution times have been successfully logged.")
                except Exception as e:
                    print(f"Error logging DAG {response_content.get('dag_id')} to CSV: {e}")

                try:
                    with open(args.task_exec_result_jsonl_path, 'a', encoding='utf-8') as f:
                        json_str = json.dumps(response_content, ensure_ascii=False)
                        f.write(json_str + '\n')
                    print(f"ğŸ’  ä»»åŠ¡ç»“æœæ•°æ®å·²æˆåŠŸè¿½åŠ åˆ°: {args.task_exec_result_jsonl_path}")
                except Exception as e:
                    print(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
                # =================================================================

            except Exception as e:
                print(f"âŒ [Dispatcher] ä»»åŠ¡ {uid} çš„æ‰§è¡Œæˆ–ç»“æœå¤„ç†åç¨‹å¤±è´¥: {e}\n{traceback.format_exc()}")
                if uid in self.active_tasks:
                    self.active_tasks[uid].update({"status": "Failed", "error": str(e)})
        
        print(f"ğŸ”µ æœºå™¨ [{host}] å·²é‡Šæ”¾ï¼Œä»»åŠ¡ {uid} å¤„ç†å®Œæ¯•ã€‚")

    async def dispatch_and_wait(self, msg: Msg):
        """è¿™ä¸ªå‡½æ•°ç°åœ¨åªè´Ÿè´£æ´¾å‘ï¼Œå¹¶ä¸ºåå°ä»»åŠ¡å‡†å¤‡å¥½æœºå™¨é”"""
        uid = "unknown"
        try:
            task_info = ast.literal_eval(msg.content)
            print("ğŸ’¨ Now we deal with task_info")
            uid = task_info.get("uuid")
            workflow_type = task_info.get("type")

            worker_id = self.router.get_next_worker_id(workflow_type)
            if not worker_id:
                raise ValueError(f"æœªæ‰¾åˆ°å¤„ç† '{workflow_type}' ç±»å‹çš„Workeræ± ã€‚")

            agent_name, address = worker_id.split('@')
            host, port_str = address.split(':')
            port = int(port_str)
            
            async with self.lock_management_lock:
                if host not in self.machine_locks:
                    self.machine_locks[host] = asyncio.Lock()
            machine_lock_for_task = self.machine_locks[host]
            
            print(f"ğŸš€ [Dispatcher] ä»»åŠ¡ {uid} å·²æäº¤è‡³åå°æ‰§è¡Œé˜Ÿåˆ— -> {worker_id}")

            asyncio.create_task(self._execute_and_process_result(
                msg.content, uid, workflow_type, worker_id, agent_name, host, port,
                machine_lock=machine_lock_for_task
            ))
        except Exception as e:
            print(f"âŒ [Dispatcher] ä»»åŠ¡ {uid} åœ¨æ´¾å‘é˜¶æ®µå¤±è´¥: {e}\n{traceback.format_exc()}")
            if uid != 'unknown' and uid in self.active_tasks:
                 self.active_tasks[uid].update({"status": "Failed", "error": f"Dispatch error: {e}"})

def create_flask_app(loop: asyncio.AbstractEventLoop, task_loader: TaskLoader, active_tasks: Dict, final_results: Dict) -> Flask:
    app = Flask(__name__)
    @app.route("/submit_dag", methods=["POST"])
    def submit_dag() -> Any:
        payload = request.get_json()
        if not payload: return jsonify({"error": "Request body must be JSON."}), 400
        print(f"ğŸ‘¹ æ¥æ”¶åˆ°ä»»åŠ¡æäº¤è¯·æ±‚...")
        results = []
        sub_time = payload["sub_time"]
        for dag_id, dag_type, dag_source, supplementary_files in zip(payload["dag_ids"], payload["dag_types"], payload["dag_sources"], payload["dag_supplementary_files"]):
            try:
                message = task_loader.load_workflow_message(dag_id, dag_type, dag_source, supplementary_files, sub_time)
                uid = ast.literal_eval(message.content).get("uuid")
                active_tasks[uid] = {"status": "Queued", "dag_id": dag_id}
                asyncio.run_coroutine_threadsafe(dag_queue.put(message), loop)
                results.append({"dag_id": dag_id, "uuid": uid, "status": "scheduled"})
            except Exception as e:
                results.append({"dag_id": dag_id, "error": str(e)})
        print(f"ğŸ“¦ [Flask] æ”¶åˆ°å¹¶å¤„ç†äº† {len(payload['dag_ids'])} ä¸ªDAGçš„æäº¤è¯·æ±‚ã€‚")
        return jsonify({"submitted": results}), 200

    @app.route("/dag_status/<dag_uuid>", methods=["GET"])
    def dag_status(dag_uuid: str) -> Any:
        return jsonify(active_tasks.get(dag_uuid, {"error": "DAG not found"}))

    @app.route("/get_final_result/<dag_uuid>", methods=["GET"])
    def get_final_result(dag_uuid: str) -> Any:
        return jsonify(final_results.get(dag_uuid, {"error": "Result not found"}))
    return app

async def background_dispatcher_loop(dispatcher: Dispatcher):
    """åå°è°ƒåº¦å¾ªç¯ï¼Œä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡å¹¶å¹¶å‘åœ°å¤„ç†ã€‚"""
    print("ğŸš€ åå°è°ƒåº¦å¾ªç¯å·²å¯åŠ¨...")
    while True:
        msg = await dag_queue.get()
        await dispatcher.dispatch_and_wait(msg)
        await asyncio.sleep(0.05)  # æ§åˆ¶è°ƒåº¦é¢‘ç‡ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è°ƒåº¦

def main():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£åˆå§‹åŒ–å¹¶å¯åŠ¨æ‰€æœ‰æœåŠ¡ã€‚"""
    csv_head = ['dag_id', 'uuid', 'sub_time', 'arrival_time', 'start_exec_time', 'finish_exec_time', 'exec_time', 'leave_time', 'completion_time', 'response_time']
    if not os.path.exists(args.task_exec_time_csv_path):
        with open(args.task_exec_time_csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(csv_head)
        print(f"'{args.task_exec_time_csv_path}' created with headers.")
    if not os.path.exists(args.task_exec_result_jsonl_path):
        with open(args.task_exec_result_jsonl_path, 'w') as f:
            pass
        print(f"'{args.task_exec_result_jsonl_path}' created.")

    executor = ThreadPoolExecutor(max_workers= os.cpu_count()* 4)  # ä½¿ç”¨çº¿ç¨‹æ± æ¥å¤„ç†å­è¿›ç¨‹çš„è°ƒç”¨
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    active_tasks: Dict[str, Any] = {}
    final_results: Dict[str, Any] = {}
    router = RoundRobinRouter(args.agent_pools)
    dispatcher = Dispatcher(router, active_tasks, final_results, executor=executor)
    task_loader = TaskLoader(args)

    app = create_flask_app(loop, task_loader, active_tasks, final_results)
    flask_thread = threading.Thread(target=lambda: app.run(host=args.host, port=args.port, debug=False, use_reloader=False), daemon=True)
    flask_thread.start()
    print(f"âœ… Flask API æœåŠ¡å·²åœ¨ http://{args.host}:{args.port} ä¸Šå¯åŠ¨")

    try:
        print("ä¸»çº¿ç¨‹å·²å¯åŠ¨ï¼Œæ­£åœ¨è¿è¡Œ asyncio äº‹ä»¶å¾ªç¯æ¥è°ƒåº¦ä»»åŠ¡...")
        loop.run_until_complete(
            background_dispatcher_loop(dispatcher)
        )
    except KeyboardInterrupt:
        print("\næ”¶åˆ°é€€å‡ºä¿¡å·...")
    finally:
        print("æ­£åœ¨å…³é—­è¿›ç¨‹æ± ...")
        executor.shutdown(wait=True)
        print("æ­£åœ¨å…³é—­äº‹ä»¶å¾ªç¯...")
        loop.close()
        print("æœåŠ¡å·²å…³é—­ã€‚")

if __name__ == "__main__":
    main()
