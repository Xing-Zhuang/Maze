def task1_start_receive_task(args, dag_id, question, supplementary_files):
    """Task 1: æ¥æ”¶ä»»åŠ¡ï¼Œç¡®è®¤æ ¸å¿ƒå‚æ•°ã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 1: Starting... Verifying initial parameters.")
        print(f"  -> Received dag_id: {dag_id}, question: '{question[:100]}...', {len(supplementary_files)} files.")
        # è¯»å– questions.txt å†…å®¹ï¼Œä½œä¸º questions
        questions = supplementary_files['questions.txt'].decode('utf-8') if 'questions.txt' in supplementary_files else ""
        print(f"Loaded {len(questions.splitlines())} questions from questions.txt")
        end_time = time.time()
        return {
            "dag_id": dag_id,
            "question": question,
            "questions": questions,  # æ–°å¢å­—æ®µ
            "supplementary_files": supplementary_files,
            "args": args,
            "start_time": start_time,
            "end_time": end_time
        }
    except Exception as e:
        end_time = time.time()
        return {
            "dag_id": dag_id, "question": None, "questions": None, "supplementary_files": None, "args": args,
            "start_time": start_time, "end_time": end_time
        }

def task2_read_file(args, dag_id, supplementary_files):
    """Task 2: è¯»å–æ–‡ä»¶å†…å®¹ã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 2: Reading file content...")
        if not supplementary_files or 'context.txt' not in supplementary_files:
            raise ValueError("supplementary_files is None or 'context.txt' is missing.")
            
        document_content = supplementary_files['context.txt'].decode('utf-8')
        print(f"âœ… Task 2: Finished. Text content length: {len(document_content)}")
        end_time = time.time()
        return {
            "dag_id": dag_id, "document_content": document_content, "args": args,
            "start_time": start_time, "end_time": end_time
        }
    except Exception as e:
        end_time = time.time()
        return {
            "dag_id": dag_id, "document_content": None, "args": args,
            "start_time": start_time, "end_time": end_time
        }

def task3a_extract_text_content(args, dag_id, document_content):
    """Task 3a: (å¹¶è¡Œ) æ–‡æœ¬å†…å®¹æ ‡å‡†åŒ–å¤„ç† (CPUå¯†é›†å‹)ã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 3a: Normalizing extracted text content...")
        if not document_content:
            raise ValueError("document_content is None or empty.")
            
        unique_lines = [line.strip() for line in document_content.split('\n') if line.strip()]
        processed_text = '\n'.join(dict.fromkeys(unique_lines)) # å»é‡å¹¶ä¿æŒé¡ºåº
        
        print(f"âœ… Task 3a: Text normalization complete. Length from {len(document_content)} to {len(processed_text)}.")
        end_time = time.time()
        return {
            "dag_id": dag_id, "extracted_text": processed_text, "args": args,
            "start_time": start_time, "end_time": end_time
        }
    except Exception as e:
        end_time = time.time()
        return {
            "dag_id": dag_id, "extracted_text": None, "args": args,
            "start_time": start_time, "end_time": end_time
        }

def task3b_llm_process_extract_structure_info(args, dag_id, document_content, vllm_manager= None, backend= "huggingface"):
    """Task 3b: (å¹¶è¡Œ) ä½¿ç”¨LLMåˆ†ææ–‡æ¡£ç»“æ„ã€‚"""
    import os, gc, time, torch
    import asyncio
    import aiohttp
    from typing import Optional, Dict, List, Tuple, Any
    start_time = time.time()
    async def _query_single_vllm_endpoint(
        session: aiohttp.ClientSession,
        chat_url: str,
        payload: Dict[str, Any]
    ) -> str:
        """å¼‚æ­¥å‘é€å•ä¸ªè¯·æ±‚åˆ°vLLMçš„coroutineã€‚"""
        try:
            async with session.post(chat_url, json=payload, timeout=3600) as response:
                response.raise_for_status()
                response_data = await response.json()
                return response_data['choices'][0]['message']['content'].strip()
        except Exception as e:
            error_msg = f"vLLM async request failed: {str(e)}"
            print(f"[bold red]{error_msg}")
            return error_msg

    async def _query_vllm_batch_async(
        api_url: str,
        model_alias: str,
        messages_list: List[List[Dict[str, str]]],
        temperature: float,
        max_token: int,
        top_p: float,
        repetition_penalty: float
    ) -> List[str]:
        """ä½¿ç”¨ aiohttp å¹¶å‘æ‰§è¡Œæ‰€æœ‰vLLMè¯·æ±‚ã€‚"""
        chat_url = f"{api_url.strip('/')}/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        
        async with aiohttp.ClientSession(headers=headers) as session:
            tasks = []
            for messages in messages_list:
                payload = {
                    "model": model_alias,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_token,
                    "top_p": top_p,
                    "repetition_penalty": repetition_penalty,
                }
                tasks.append(_query_single_vllm_endpoint(session, chat_url, payload))
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
            all_responses = await asyncio.gather(*tasks)
            return all_responses

    def query_vllm_batch(
        api_url: str,
        model_alias: str,
        messages_list: List[List[Dict[str, str]]],
        temperature: float = 0.6,
        max_token: int = 1024,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1
    ) -> Tuple[Dict, List[str]]:
        """
        [æ–°å¢] ä½¿ç”¨vLLMæœåŠ¡æ‰¹é‡å¤„ç†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚
        - åˆ©ç”¨asyncioå’Œaiohttpå®ç°é«˜å¹¶å‘è¯·æ±‚ï¼Œè¾¾åˆ°æ‰¹é‡å¤„ç†çš„æ•ˆæœã€‚
        """
        print(f"  -> Starting vLLM batch processing: {len(messages_list)} prompts concurrently.")
        
        # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
        batch_answers = asyncio.run(_query_vllm_batch_async(
            api_url, model_alias, messages_list, temperature, max_token, top_p, repetition_penalty
        ))
        print("  -> vLLM batch processing finished.")
        return batch_answers

    def _query_llm_single(model_folder, model_name, messages, temperature, max_token, top_p, repetition_penalty):
        """A simplified LLM query for a single prompt."""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        model, tokenizer = None, None
        try:
            tokenizer_path = os.path.join(model_folder, "Qwen/Qwen3-32B")
            model_path = os.path.join(model_folder, model_name)
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map="cuda", offload_state_dict= False,trust_remote_code=True)
            
            prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
            outputs = model.generate(**inputs, max_new_tokens=max_token, temperature=temperature, top_p=top_p, repetition_penalty=repetition_penalty)
            return tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0]
        finally:
            del model
            del tokenizer
            del inputs
            del outputs
            gc.collect()
            torch.cuda.empty_cache()
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                torch.cuda.reset_max_memory_allocated()
                torch.cuda.reset_peak_memory_stats()

    try:
        print("âœ… Task 3b: Analyzing document structure with LLM...")
        if not document_content:
            raise ValueError("document_content is None or empty.")
        
        structure_prompt = f"Please analyze the structure of the following document and provide a brief summary.\n\nDocument (first 3000 chars):\n{document_content[:3000]}"
        messages = [{"role": "user", "content": structure_prompt}]
        
        if backend == "vllm":
            batch_results = query_vllm_batch(
                api_url= vllm_manager.get_next_endpoint("qwen3-32b"),
                model_alias= "qwen3-32b",
                messages_list= [messages],
                temperature= args.temperature,
                max_token= args.max_token,
                top_p= args.top_p,
                repetition_penalty= args.repetition_penalty,
            )
            
            # 2. å®‰å…¨æ€§æ£€æŸ¥ï¼šç¡®ä¿åˆ—è¡¨ä¸æ˜¯ç©ºçš„
            if not batch_results:
                raise ValueError("vLLM aPI call returned an empty or invalid result.")
                
            # 3. å®‰å…¨åœ°æå–ç¬¬ä¸€ä¸ªå…ƒç´ 
            structure_summary = batch_results[0]
        else:
            structure_summary = _query_llm_single(
                model_folder=args.model_folder, model_name="Qwen/Qwen3-32B", messages=messages,
                temperature=args.temperature, max_token=args.max_token, top_p=args.top_p, repetition_penalty=args.repetition_penalty
            )
        
        print("âœ… Task 3b: Structure analysis finished.")
        end_time = time.time()
        return {"dag_id": dag_id, "doc_structure": structure_summary, "args": args,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "doc_structure": f"Error during structure analysis: {e}", "args": args,
                "start_time": start_time, "end_time": end_time}


def task3c_load_questions_batch(args, dag_id, questions):
    """Task 3c: (å¹¶è¡Œ) åŠ è½½é—®é¢˜å¹¶åˆ†æ‰¹ã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 3c: Loading and batching questions...")
        if not questions:
            raise ValueError("Questions string is None or empty.")
        
        questions_list = [q.strip() for q in questions.split('\n') if q.strip()]
        print(f"Loaded {len(questions_list)} questions for batching.")
        num_questions = len(questions_list)
        batch1_size = int(0.2 * num_questions)
        batch2_size = int(0.2 * num_questions)
        batches = [
            questions_list[:batch1_size],
            questions_list[batch1_size : batch1_size + batch2_size],
            questions_list[batch1_size + batch2_size:]
        ]
        
        while len(batches) < 3:
            batches.append([])
            
        print(f"âœ… Task 3c: Loaded {len(questions_list)} questions into {len(batches)} batches.")
        end_time = time.time()
        return {"dag_id": dag_id, "question_batches": batches, "args": args,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "question_batches": None, "args": args,
                "start_time": start_time, "end_time": end_time}


def task4a_merge_document_analysis(args, dag_id, extracted_text, doc_structure):
    """Task 4a: (åˆå¹¶ç‚¹) åˆå¹¶æ–‡æ¡£å†…å®¹å’Œç»“æ„ã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 4a: Merging document analysis results...")
        if not all([extracted_text, doc_structure]):
             raise ValueError("Upstream data (extracted_text or doc_structure) is missing.")

        merged_analysis = {"content": extracted_text, "structure": doc_structure}
        print("âœ… Task 4a: Document analysis merged successfully.")
        end_time = time.time()
        return {"dag_id": dag_id, "merged_document_analysis": merged_analysis, "args": args,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "merged_document_analysis": None, "args": args,
                "start_time": start_time, "end_time": end_time}

def task4b_prepare_qa_context(args, dag_id, merged_document_analysis, question_batches):
    """Task 4b: (åˆå¹¶ç‚¹) å‡†å¤‡QAä¸Šä¸‹æ–‡ã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 4b: Preparing final QA context...")
        if not all([merged_document_analysis, question_batches is not None]):
            raise ValueError("Upstream data (merged_document_analysis or question_batches) is missing.")

        qa_context = {
            "document_content": merged_document_analysis["content"][:12000],
            "document_structure": merged_document_analysis["structure"],
            "question_batches": question_batches
        }
        
        print("âœ… Task 4b: Final QA context is ready.")
        end_time = time.time()
        return {"dag_id": dag_id, "qa_context": qa_context, "args": args,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "qa_context": None, "args": args,
                "start_time": start_time, "end_time": end_time}

def task5a_llm_process_batch_1(args, dag_id, qa_context, vllm_manager= None, backend= "huggingface"):
    """Task 5a: å¤„ç†ç¬¬1æ‰¹é—®é¢˜ã€‚"""
    import time
    start_time = time.time()
    
    def _qa_processing_worker(task_name, args, qa_context, batch_index):
        """ä¸€ä¸ªé€šç”¨çš„QAå·¥ä½œå‡½æ•°ï¼Œå¤„ç†ä¸€ä¸ªé—®é¢˜æ‰¹æ¬¡ã€‚å®ƒåŒ…å«äº†æ‰€æœ‰ä¾èµ–é¡¹ä»¥ç¡®ä¿ç‹¬ç«‹æ€§ã€‚"""
        import os, gc, time, math, torch
        from typing import Optional, Dict, List, Tuple, Any
        import asyncio
        import aiohttp  
        async def _query_single_vllm_endpoint(
            session: aiohttp.ClientSession,
            chat_url: str,
            payload: Dict[str, Any]
        ) -> str:
            """å¼‚æ­¥å‘é€å•ä¸ªè¯·æ±‚åˆ°vLLMçš„coroutineã€‚"""
            try:
                async with session.post(chat_url, json=payload, timeout=3600) as response:
                    response.raise_for_status()
                    response_data = await response.json()
                    return response_data['choices'][0]['message']['content'].strip()
            except Exception as e:
                error_msg = f"vLLM async request failed: {str(e)}"
                print(f"[bold red]{error_msg}")
                return error_msg
            
        async def _query_vllm_batch_async(
            api_url: str,
            model_alias: str,
            messages_list: List[List[Dict[str, str]]],
            temperature: float,
            max_token: int,
            top_p: float,
            repetition_penalty: float
        ) -> List[str]:
            """ä½¿ç”¨ aiohttp å¹¶å‘æ‰§è¡Œæ‰€æœ‰vLLMè¯·æ±‚ã€‚"""
            chat_url = f"{api_url.strip('/')}/v1/chat/completions"
            headers = {"Content-Type": "application/json"}
            
            async with aiohttp.ClientSession(headers=headers) as session:
                tasks = []
                for messages in messages_list:
                    payload = {
                        "model": model_alias,
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_token,
                        "top_p": top_p,
                        "repetition_penalty": repetition_penalty,
                    }
                    tasks.append(_query_single_vllm_endpoint(session, chat_url, payload))
                
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
                all_responses = await asyncio.gather(*tasks)
                return all_responses

        def query_vllm_batch(
            api_url: str,
            model_alias: str,
            messages_list: List[List[Dict[str, str]]],
            temperature: float = 0.6,
            max_token: int = 1024,
            top_p: float = 0.9,
            repetition_penalty: float = 1.1
        ) -> Tuple[Dict, List[str]]:
            """
            [æ–°å¢] ä½¿ç”¨vLLMæœåŠ¡æ‰¹é‡å¤„ç†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚
            - åˆ©ç”¨asyncioå’Œaiohttpå®ç°é«˜å¹¶å‘è¯·æ±‚ï¼Œè¾¾åˆ°æ‰¹é‡å¤„ç†çš„æ•ˆæœã€‚
            """
            print(f"  -> Starting vLLM batch processing: {len(messages_list)} prompts concurrently.")
            
            # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
            batch_answers = asyncio.run(_query_vllm_batch_async(
                api_url, model_alias, messages_list, temperature, max_token, top_p, repetition_penalty
            ))
            
            print("  -> vLLM batch processing finished.")
            return batch_answers

        def _query_llm_batch(model_folder: str, model_name: str, messages_list: List[List[Dict[str, str]]],
                            temperature: float, max_token: int, top_p: float, repetition_penalty: float, batch_size: int):
            from transformers import AutoTokenizer, AutoModelForCausalLM
            model, tokenizer = None, None
            try:
                print(f"  -> [{task_name}] Loading batch LLM: {model_name}...")
                tokenizer_path = os.path.join(model_folder, "Qwen/Qwen3-32B")
                model_path = os.path.join(model_folder, model_name)
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, padding_side='left', trust_remote_code=True)
                if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
                model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map="cuda", offload_state_dict= False, trust_remote_code=True)
                
                prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages_list]
                all_responses = []
                for i in range(0, len(prompts), batch_size):
                    batch_prompts = prompts[i:i + batch_size]
                    inputs = tokenizer(batch_prompts, return_tensors='pt', padding=True).to(model.device)
                    outputs = model.generate(**inputs, max_new_tokens=max_token, temperature=temperature, pad_token_id=tokenizer.eos_token_id, top_p=top_p, repetition_penalty=repetition_penalty)
                    all_responses.extend(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True))
                    del inputs, outputs
                return all_responses
            finally:
                del model, tokenizer
                gc.collect()
                torch.cuda.empty_cache()
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.reset_max_memory_allocated()
                    torch.cuda.reset_peak_memory_stats()
                print(f"  -> [{task_name}] Batch LLM resources released.")

        print(f"âš™ï¸  Starting QA processing for {task_name}...")
        if not qa_context or batch_index >= len(qa_context["question_batches"]):
            print(f"  -> {task_name} has invalid context or batch index. Skipping.")
            return []

        questions_in_batch = qa_context["question_batches"][batch_index]
        if not questions_in_batch:
            print(f"  -> {task_name} batch is empty. Skipping.")
            return []

        # æ–°å¢ï¼šå°† context æ‹†è¡Œï¼Œä¸é—®é¢˜ä¸€ä¸€å¯¹åº”
        context_lines = qa_context["document_content"].split('\n')

        messages_list = []
        for idx, q in enumerate(questions_in_batch):
            context_for_this_question = context_lines[idx] if idx < len(context_lines) else ""
            messages_list.append([
                {"role": "user", "content": f"""Based on the following document content, directly answer the question. Only output the most concise answer, do not explain or repeat the question.
Document structure summary:
{qa_context['document_structure']}

Document content:
{context_for_this_question}

Question: {q}
Answer:"""}
            ])
        print(f"  -> {task_name} has {len(messages_list)} questions to process in this batch.")
        if backend == "vllm":
            # ä½¿ç”¨vLLMæœåŠ¡è¿›è¡Œæ‰¹é‡å¤„ç†
            batch_answers = query_vllm_batch(
                api_url= vllm_manager.get_next_endpoint("qwen3-32b"),
                model_alias= "qwen3-32b",
                messages_list= messages_list,
                temperature= args.temperature,
                max_token= getattr(args, "max_token", 1024),
                top_p= args.top_p,
                repetition_penalty= args.repetition_penalty
            )
        else:
            batch_answers = _query_llm_batch(
                model_folder=args.model_folder, model_name="Qwen/Qwen3-32B", messages_list=messages_list,
                temperature=args.temperature, max_token=getattr(args, "max_token", 1024), top_p=args.top_p,
                repetition_penalty=args.repetition_penalty, batch_size=getattr(args, "text_batch_size", 1)
            )
        print(f"âœ… {task_name} finished. Answered {len(batch_answers)} questions.")
        return batch_answers
    try:
        results = _qa_processing_worker("task5a", args, qa_context, 0)
        end_time = time.time()
        return {"dag_id": dag_id, "batch1_answers": results,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "batch1_answers": None,
                "start_time": start_time, "end_time": end_time}

def task5b_llm_process_batch_2(args, dag_id, qa_context, vllm_manager= None, backend= "huggingface"):
    """Task 5b: å¤„ç†ç¬¬2æ‰¹é—®é¢˜ã€‚"""
    import time
    from typing import Optional, Dict, List, Tuple, Any
    start_time = time.time()

    def _qa_processing_worker(task_name, args, qa_context, batch_index):
        """ä¸€ä¸ªé€šç”¨çš„QAå·¥ä½œå‡½æ•°ï¼Œå¤„ç†ä¸€ä¸ªé—®é¢˜æ‰¹æ¬¡ã€‚å®ƒåŒ…å«äº†æ‰€æœ‰ä¾èµ–é¡¹ä»¥ç¡®ä¿ç‹¬ç«‹æ€§ã€‚"""
        import os, gc, time, math, torch
        from typing import List, Dict
        import asyncio
        import aiohttp
        async def _query_single_vllm_endpoint(
            session: aiohttp.ClientSession,
            chat_url: str,
            payload: Dict[str, Any]
        ) -> str:
            """å¼‚æ­¥å‘é€å•ä¸ªè¯·æ±‚åˆ°vLLMçš„coroutineã€‚"""
            try:
                async with session.post(chat_url, json=payload, timeout=3600) as response:
                    response.raise_for_status()
                    response_data = await response.json()
                    return response_data['choices'][0]['message']['content'].strip()
            except Exception as e:
                error_msg = f"vLLM async request failed: {str(e)}"
                print(f"[bold red]{error_msg}")
                return error_msg
            
        async def _query_vllm_batch_async(
            api_url: str,
            model_alias: str,
            messages_list: List[List[Dict[str, str]]],
            temperature: float,
            max_token: int,
            top_p: float,
            repetition_penalty: float
        ) -> List[str]:
            """ä½¿ç”¨ aiohttp å¹¶å‘æ‰§è¡Œæ‰€æœ‰vLLMè¯·æ±‚ã€‚"""
            chat_url = f"{api_url.strip('/')}/v1/chat/completions"
            headers = {"Content-Type": "application/json"}
            
            async with aiohttp.ClientSession(headers=headers) as session:
                tasks = []
                for messages in messages_list:
                    payload = {
                        "model": model_alias,
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_token,
                        "top_p": top_p,
                        "repetition_penalty": repetition_penalty,
                    }
                    tasks.append(_query_single_vllm_endpoint(session, chat_url, payload))
                
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
                all_responses = await asyncio.gather(*tasks)
                return all_responses

        def query_vllm_batch(
            api_url: str,
            model_alias: str,
            messages_list: List[List[Dict[str, str]]],
            temperature: float = 0.6,
            max_token: int = 1024,
            top_p: float = 0.9,
            repetition_penalty: float = 1.1
        ) -> Tuple[Dict, List[str]]:
            """
            [æ–°å¢] ä½¿ç”¨vLLMæœåŠ¡æ‰¹é‡å¤„ç†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚
            - åˆ©ç”¨asyncioå’Œaiohttpå®ç°é«˜å¹¶å‘è¯·æ±‚ï¼Œè¾¾åˆ°æ‰¹é‡å¤„ç†çš„æ•ˆæœã€‚
            """
            print(f"  -> Starting vLLM batch processing: {len(messages_list)} prompts concurrently.")
            
            # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
            batch_answers = asyncio.run(_query_vllm_batch_async(
                api_url, model_alias, messages_list, temperature, max_token, top_p, repetition_penalty
            ))
            
            print("  -> vLLM batch processing finished.")
            return batch_answers

        def _query_llm_batch(model_folder: str, model_name: str, messages_list: List[List[Dict[str, str]]],
                            temperature: float, max_token: int, top_p: float, repetition_penalty: float, batch_size: int):
            from transformers import AutoTokenizer, AutoModelForCausalLM
            model, tokenizer = None, None
            try:
                print(f"  -> [{task_name}] Loading batch LLM: {model_name}...")
                tokenizer_path = os.path.join(model_folder, "Qwen/Qwen3-32B")
                model_path = os.path.join(model_folder, model_name)
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, padding_side='left', trust_remote_code=True)
                if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
                model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map="cuda", offload_state_dict= False, trust_remote_code=True)
                
                prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages_list]
                all_responses = []
                for i in range(0, len(prompts), batch_size):
                    batch_prompts = prompts[i:i + batch_size]
                    inputs = tokenizer(batch_prompts, return_tensors='pt', padding=True).to(model.device)
                    outputs = model.generate(**inputs, max_new_tokens=max_token, temperature=temperature, pad_token_id=tokenizer.eos_token_id, top_p=top_p, repetition_penalty=repetition_penalty)
                    all_responses.extend(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True))
                    del inputs, outputs
                return all_responses
            finally:
                del model, tokenizer
                gc.collect()
                torch.cuda.empty_cache()
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.reset_max_memory_allocated()
                    torch.cuda.reset_peak_memory_stats()
                print(f"  -> [{task_name}] Batch LLM resources released.")

        print(f"âš™ï¸  Starting QA processing for {task_name}...")
        if not qa_context or batch_index >= len(qa_context["question_batches"]):
            print(f"  -> {task_name} has invalid context or batch index. Skipping.")
            return []

        questions_in_batch = qa_context["question_batches"][batch_index]
        if not questions_in_batch:
            print(f"  -> {task_name} batch is empty. Skipping.")
            return []

        # æ–°å¢ï¼šå°† context æ‹†è¡Œï¼Œä¸é—®é¢˜ä¸€ä¸€å¯¹åº”
        context_lines = qa_context["document_content"].split('\n')

        messages_list = []
        for idx, q in enumerate(questions_in_batch):
            context_for_this_question = context_lines[idx] if idx < len(context_lines) else ""
            messages_list.append([
                {"role": "user", "content": f"""Based on the following document content, directly answer the question. Only output the most concise answer, do not explain or repeat the question.
Document structure summary:
{qa_context['document_structure']}

Document content:
{context_for_this_question}

Question: {q}
Answer:"""}
            ])
        if backend == "vllm":
            # ä½¿ç”¨vLLMæœåŠ¡è¿›è¡Œæ‰¹é‡å¤„ç†
            batch_answers = query_vllm_batch(
                api_url= vllm_manager.get_next_endpoint("qwen3-32b"),
                model_alias= "qwen3-32b",
                messages_list= messages_list,
                temperature= args.temperature,
                max_token= getattr(args, "max_token", 1024),
                top_p= args.top_p,
                repetition_penalty= args.repetition_penalty
            )
        else:
            batch_answers = _query_llm_batch(
                model_folder=args.model_folder, model_name="Qwen/Qwen3-32B", messages_list=messages_list,
                temperature=args.temperature, max_token=getattr(args, "max_token", 1024), top_p=args.top_p,
                repetition_penalty=args.repetition_penalty, batch_size=getattr(args, "text_batch_size", 1)
            )
        print(f"âœ… {task_name} finished. Answered {len(batch_answers)} questions.")
        return batch_answers
    try:
        results = _qa_processing_worker("task5b", args, qa_context, 1)
        end_time = time.time()
        return {"dag_id": dag_id, "batch2_answers": results,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "batch2_answers": None,
                "start_time": start_time, "end_time": end_time}

def task5c_llm_process_batch_3(args, dag_id, qa_context, vllm_manager= None, backend= "huggingface"):
    """Task 5c: å¤„ç†ç¬¬3æ‰¹é—®é¢˜ã€‚"""
    import time
    from typing import Optional, Dict, List, Tuple, Any
    start_time = time.time()
    
    def _qa_processing_worker(task_name, args, qa_context, batch_index):
        """ä¸€ä¸ªé€šç”¨çš„QAå·¥ä½œå‡½æ•°ï¼Œå¤„ç†ä¸€ä¸ªé—®é¢˜æ‰¹æ¬¡ã€‚å®ƒåŒ…å«äº†æ‰€æœ‰ä¾èµ–é¡¹ä»¥ç¡®ä¿ç‹¬ç«‹æ€§ã€‚"""
        import os, gc, time, math, torch
        from typing import List, Dict
        import asyncio
        import aiohttp
        async def _query_single_vllm_endpoint(
            session: aiohttp.ClientSession,
            chat_url: str,
            payload: Dict[str, Any]
        ) -> str:
            """å¼‚æ­¥å‘é€å•ä¸ªè¯·æ±‚åˆ°vLLMçš„coroutineã€‚"""
            try:
                async with session.post(chat_url, json=payload, timeout=3600) as response:
                    response.raise_for_status()
                    response_data = await response.json()
                    return response_data['choices'][0]['message']['content'].strip()
            except Exception as e:
                error_msg = f"vLLM async request failed: {str(e)}"
                print(f"[bold red]{error_msg}")
                return error_msg
            
        async def _query_vllm_batch_async(
            api_url: str,
            model_alias: str,
            messages_list: List[List[Dict[str, str]]],
            temperature: float,
            max_token: int,
            top_p: float,
            repetition_penalty: float
        ) -> List[str]:
            """ä½¿ç”¨ aiohttp å¹¶å‘æ‰§è¡Œæ‰€æœ‰vLLMè¯·æ±‚ã€‚"""
            chat_url = f"{api_url.strip('/')}/v1/chat/completions"
            headers = {"Content-Type": "application/json"}
            
            async with aiohttp.ClientSession(headers=headers) as session:
                tasks = []
                for messages in messages_list:
                    payload = {
                        "model": model_alias,
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_token,
                        "top_p": top_p,
                        "repetition_penalty": repetition_penalty,
                    }
                    tasks.append(_query_single_vllm_endpoint(session, chat_url, payload))
                
                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
                all_responses = await asyncio.gather(*tasks)
                return all_responses

        def query_vllm_batch(
            api_url: str,
            model_alias: str,
            messages_list: List[List[Dict[str, str]]],
            temperature: float = 0.6,
            max_token: int = 1024,
            top_p: float = 0.9,
            repetition_penalty: float = 1.1
        ) -> Tuple[Dict, List[str]]:
            """
            [æ–°å¢] ä½¿ç”¨vLLMæœåŠ¡æ‰¹é‡å¤„ç†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚
            - åˆ©ç”¨asyncioå’Œaiohttpå®ç°é«˜å¹¶å‘è¯·æ±‚ï¼Œè¾¾åˆ°æ‰¹é‡å¤„ç†çš„æ•ˆæœã€‚
            """
            print(f"  -> Starting vLLM batch processing: {len(messages_list)} prompts concurrently.")
            
            # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
            batch_answers = asyncio.run(_query_vllm_batch_async(
                api_url, model_alias, messages_list, temperature, max_token, top_p, repetition_penalty
            ))
            
            print("  -> vLLM batch processing finished.")
            return batch_answers

        def _query_llm_batch(model_folder: str, model_name: str, messages_list: List[List[Dict[str, str]]],
                            temperature: float, max_token: int, top_p: float, repetition_penalty: float, batch_size: int):
            from transformers import AutoTokenizer, AutoModelForCausalLM
            model, tokenizer = None, None
            try:
                print(f"  -> [{task_name}] Loading batch LLM: {model_name}...")
                tokenizer_path = os.path.join(model_folder, "Qwen/Qwen3-32B")
                model_path = os.path.join(model_folder, model_name)
                tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, padding_side='left', trust_remote_code=True)
                if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
                model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map="cuda", offload_state_dict= False, trust_remote_code=True)
                
                prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages_list]
                all_responses = []
                for i in range(0, len(prompts), batch_size):
                    batch_prompts = prompts[i:i + batch_size]
                    inputs = tokenizer(batch_prompts, return_tensors='pt', padding=True).to(model.device)
                    outputs = model.generate(**inputs, max_new_tokens=max_token, temperature=temperature, pad_token_id=tokenizer.eos_token_id, top_p=top_p, repetition_penalty=repetition_penalty)
                    all_responses.extend(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True))
                    del inputs, outputs
                return all_responses
            finally:
                del model, tokenizer
                gc.collect()
                torch.cuda.empty_cache()
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.reset_max_memory_allocated()
                    torch.cuda.reset_peak_memory_stats()
                print(f"  -> [{task_name}] Batch LLM resources released.")

        print(f"âš™ï¸  Starting QA processing for {task_name}...")
        if not qa_context or batch_index >= len(qa_context["question_batches"]):
            print(f"  -> {task_name} has invalid context or batch index. Skipping.")
            return []

        questions_in_batch = qa_context["question_batches"][batch_index]
        if not questions_in_batch:
            print(f"  -> {task_name} batch is empty. Skipping.")
            return []

        # æ–°å¢ï¼šå°† context æ‹†è¡Œï¼Œä¸é—®é¢˜ä¸€ä¸€å¯¹åº”
        context_lines = qa_context["document_content"].split('\n')

        messages_list = []
        for idx, q in enumerate(questions_in_batch):
            context_for_this_question = context_lines[idx] if idx < len(context_lines) else ""
            messages_list.append([
                {"role": "user", "content": f"""Based on the following document content, directly answer the question. Only output the most concise answer, do not explain or repeat the question.
Document structure summary:
{qa_context['document_structure']}

Document content:
{context_for_this_question}

Question: {q}
Answer:"""}
            ])
        if backend == "vllm":
            # ä½¿ç”¨vLLMæœåŠ¡è¿›è¡Œæ‰¹é‡å¤„ç†
            batch_answers = query_vllm_batch(
                api_url= vllm_manager.get_next_endpoint("qwen3-32b"),
                model_alias= "qwen3-32b",
                messages_list= messages_list,
                temperature= args.temperature,
                max_token= getattr(args, "max_token", 1024),
                top_p= args.top_p,
                repetition_penalty= args.repetition_penalty
            )
        else:
            batch_answers = _query_llm_batch(
                model_folder=args.model_folder, model_name="Qwen/Qwen3-32B", messages_list=messages_list,
                temperature=args.temperature, max_token=getattr(args, "max_token", 1024), top_p=args.top_p,
                repetition_penalty=args.repetition_penalty, batch_size=getattr(args, "text_batch_size", 1)
            )
        print(f"âœ… {task_name} finished. Answered {len(batch_answers)} questions.")
        return batch_answers
    try:
        results = _qa_processing_worker("task5c", args, qa_context, 2)
        end_time = time.time()
        return {"dag_id": dag_id, "batch3_answers": results,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "batch3_answers": None,
                "start_time": start_time, "end_time": end_time}

def task7_merge_all_answers(args, dag_id, batch1_answers, batch2_answers, batch3_answers):
    """Task 7: (åˆå¹¶ç‚¹) åˆå¹¶æ‰€æœ‰ç­”æ¡ˆã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 7: Merging all answers from QA batches...")
        # å®‰å…¨åœ°åˆå¹¶åˆ—è¡¨ï¼Œå¤„ç†å¯èƒ½ä¸ºNoneçš„æƒ…å†µ
        final_answers = (batch1_answers or []) + (batch2_answers or []) + (batch3_answers or [])
        
        print(f"âœ… Task 7: Merged a total of {len(final_answers)} answers.")
        end_time = time.time()
        return {"dag_id": dag_id, "final_answers": final_answers, "args": args,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "final_answers": None, "args": args,
                "start_time": start_time, "end_time": end_time}

def task8_output_final_answer(args, dag_id, final_answers):
    """Task 8: è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"""
    import time
    start_time = time.time()
    try:
        print("âœ… Task 8: Formatting final output.")
        if final_answers is None:
            final_answer_text = "Workflow failed to produce answers due to an upstream error."
        else:
            final_answer_text = '\n'.join(final_answers)
            
        print(f"ğŸ Final Answer for DAG {dag_id} generated.")
        end_time = time.time()
        return {"dag_id": dag_id, "final_answer": final_answer_text,
                "start_time": start_time, "end_time": end_time}
    except Exception as e:
        end_time = time.time()
        return {"dag_id": dag_id, "final_answer": f"Error during final output formatting: {e}",
                "start_time": start_time, "end_time": end_time}