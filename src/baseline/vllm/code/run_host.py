# run_host.py (é‡å†™ä¼˜åŒ–ç‰ˆ)
import os
import csv
import json
import time
import uuid
import base64
import argparse
import asyncio
import threading
from flask import Flask, request, jsonify

# ä¾èµ–äºæ‚¨é¡¹ç›®ä¸­çš„ç°æœ‰æ–‡ä»¶
from run_worker import DAGWorker
from baseline.utils.query_loader import GaiaLoader, TBenchLoader, OpenAGILoader

# --- å…¨å±€å…±äº«èµ„æº ---
dag_queue: asyncio.Queue = None
dag_statuses: dict = {}
dag_results: dict = {}
loop: asyncio.AbstractEventLoop = None
app = Flask(__name__)
args: argparse.Namespace = None
task_loader: 'TaskLoader' = None

def str_to_bool(val):
    """å°†å­—ç¬¦ä¸² 'true' æˆ– 'false' (ä¸åŒºåˆ†å¤§å°å†™) è½¬ä¸ºå¸ƒå°”å€¼"""
    if isinstance(val, bool):
        return val
    if val.lower() in ('true', 't', 'yes', '1'):
        return True
    elif val.lower() in ('false', 'f', 'no', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

# --- å‚æ•°è§£æ (ä¸åŸæ–‡ä»¶ä¸€è‡´) ---
def parse_host_arguments():
    """è§£æHostè¿è¡Œæ‰€éœ€çš„æ‰€æœ‰å‚æ•°ã€‚"""
    parser = argparse.ArgumentParser(description="VLLM-based Host Server")
    # è·¯å¾„å’Œç«¯å£å‚æ•°
    parser.add_argument("--proj_path", type=str, default="/root/workspace/d23oa7cp420c73acue30/AgentOS", help="é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„ã€‚")
    parser.add_argument("--data_path", type=str, default="data/", help="æ•°æ®ç›®å½•ç›¸å¯¹è·¯å¾„ã€‚")
    parser.add_argument("--dag_path", type=str, default="src/baseline/workflows", help="DAG å®šä¹‰ç›®å½•ç›¸å¯¹è·¯å¾„ã€‚")
    parser.add_argument("--flask_port", type=int, default=5002, help="FlaskæœåŠ¡å™¨ç›‘å¬çš„ç«¯å£ã€‚")
    parser.add_argument("--model_folder",  default="model_cache", 
                        help="Directory for caching downloaded models and intermediate results")
    parser.add_argument("--api_url", default="https://api.siliconflow.cn/v1/chat/completions",
                        help="API endpoint URL for online model inference requests")
    parser.add_argument("--api_key", default="Bearer sk-jbkxfkvrtluiezhqcvflmvenetulbluzpshppqqqtgxzswce",
                        help="Authentication API key for accessing the online model service (format: 'Bearer <token>')")
    parser.add_argument("--temperature", default=0.6,
                        help="Sampling temperature for model output (0.0-1.0, lower = more deterministic)")
    parser.add_argument("--max_token", default= 1024,
                        help="Maximum number of tokens allowed in the model's generated output")
    parser.add_argument("--top_p", default=0.9,
                        help="Maximum number of tokens allowed in the model's generated output")
    parser.add_argument("--repetition_penalty", default=1.1,
                        help="Maximum number of tokens allowed in the model's generated output")    
    parser.add_argument("--use_online_model", type= str_to_bool, default= False,
                        help= "use online model or no use")
    parser.add_argument("--vlm_batch_size", type= int, default= 8,
                        help="Maximum number of tokens allowed in the model's generated output")
    parser.add_argument("--text_batch_size", type= int, default= 8,
                        help="Maximum number of tokens allowed in the model's generated output")  
    # VLLMé…ç½®æ–‡ä»¶è·¯å¾„å‚æ•°
    parser.add_argument("--vllm_endpoints_json", default="src/baseline/vllm/code/vllm_endpoints.json", help="VLLMæœåŠ¡é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼ˆç›¸å¯¹é¡¹ç›®è·¯å¾„ï¼‰ã€‚")
    
    # æ—¥å¿—æ–‡ä»¶è·¯å¾„å‚æ•°
    parser.add_argument("--task_exec_time_csv_path", default="src/baseline/vllm/results/task_exec_time.csv", help="ä»»åŠ¡æ‰§è¡Œæ—¶é—´è®°å½•çš„CSVæ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--task_exec_result_jsonl_path", default="src/baseline/vllm/results/task_exec_result.jsonl", help="ä»»åŠ¡æœ€ç»ˆç»“æœè®°å½•çš„JSONLæ–‡ä»¶è·¯å¾„ã€‚")
    
    parsed_args, _ = parser.parse_known_args()

    # å°†ç›¸å¯¹è·¯å¾„æ‹¼æ¥æˆç»å¯¹è·¯å¾„
    parsed_args.model_folder= os.path.join(parsed_args.proj_path, parsed_args.model_folder)
    parsed_args.data_path = os.path.join(parsed_args.proj_path, parsed_args.data_path)
    parsed_args.dag_path = os.path.join(parsed_args.proj_path, parsed_args.dag_path)
    parsed_args.vllm_endpoints_json = os.path.join(parsed_args.proj_path, parsed_args.vllm_endpoints_json)
    parsed_args.task_exec_time_csv_path = os.path.join(parsed_args.proj_path, parsed_args.task_exec_time_csv_path)
    parsed_args.task_exec_result_jsonl_path = os.path.join(parsed_args.proj_path, parsed_args.task_exec_result_jsonl_path)
    
    return parsed_args

# --- TaskLoader (ä¸åŸæ–‡ä»¶ä¸€è‡´) ---
class TaskLoader:
    """è´Ÿè´£å°†ä»»åŠ¡å…ƒæ•°æ®ï¼Œè½¬æ¢æˆåŒ…å«æ‰€æœ‰å®é™…æ•°æ®çš„å®Œæ•´ä»»åŠ¡åŒ…ã€‚"""
    def __init__(self, args: argparse.Namespace):
        self.args = args
        self.query_loader_factory = {"gaia": GaiaLoader, "tbench": TBenchLoader, "openagi": OpenAGILoader}

    def load_task_package(self, task_data: dict, sub_time: float) -> dict:
        """åŠ è½½å•ä¸ªä»»åŠ¡çš„æ‰€æœ‰æ•°æ®ã€‚"""
        dag_source = task_data.get("dag_source")
        loader_class = self.query_loader_factory.get(dag_source)
        if not loader_class:
            raise ValueError(f"æœªæ‰¾åˆ° '{dag_source}' å¯¹åº”çš„æ•°æ®åŠ è½½å™¨ã€‚")
        
        loader = loader_class(args=self.args, dag_id=task_data.get("dag_id"), dag_type=task_data.get("dag_type"), dag_source=dag_source, supplementary_files=task_data.get("supplementary_files"), sub_time=sub_time)
        
        question = loader.question
        file_paths_map = loader.get_supplementary_files()
        
        file_contents_base64 = {}
        if file_paths_map:
            for filename, file_path in file_paths_map.items():
                try:
                    with open(file_path, 'rb') as f:
                        file_contents_base64[filename] = base64.b64encode(f.read()).decode('utf-8')
                except FileNotFoundError:
                     print(f"âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡: {file_path}")

        return {"dag_id": task_data.get("dag_id"), "dag_type": task_data.get("dag_type"), "dag_source": dag_source, "question": question, "supplementary_files": file_contents_base64, "sub_time": sub_time}

# --- æ—¥å¿—è®°å½•æ ¸å¿ƒå‡½æ•° (ä¸åŸæ–‡ä»¶ä¸€è‡´) ---
log_lock = threading.Lock()
def ensure_log_files_exist():
    os.makedirs(os.path.dirname(args.task_exec_time_csv_path), exist_ok=True)
    os.makedirs(os.path.dirname(args.task_exec_result_jsonl_path), exist_ok=True)
    if not os.path.exists(args.task_exec_time_csv_path):
        with open(args.task_exec_time_csv_path, 'w', newline='') as f:
            csv.writer(f).writerow(['dag_id', 'uuid', 'sub_time', 'arrival_time', 'start_exec_time', 'finish_exec_time', 'exec_time', 'leave_time', 'completion_time', 'response_time'])
def log_to_csv(log_data: dict):
    with log_lock:
        with open(args.task_exec_time_csv_path, 'a', newline='') as f:
            csv.writer(f).writerow([log_data.get(k) for k in ['dag_id', 'uuid', 'sub_time', 'arrival_time', 'start_exec_time', 'finish_exec_time', 'exec_time', 'leave_time', 'completion_time', 'response_time']])
def log_to_jsonl(log_data: dict):
    with log_lock:
        with open(args.task_exec_result_jsonl_path, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_data, ensure_ascii=False) + '\n')

# --- API ç«¯ç‚¹å®šä¹‰ (å‚è€ƒ Autogen ç‰ˆé‡å†™) ---
@app.route("/submit_dag", methods=["POST"])
def submit_dag_endpoint():
    payload = request.get_json()
    if not payload:
        return jsonify({"error": "Request body must be JSON."}), 400

    dag_ids = payload.get("dag_ids", [])
    dag_sources = payload.get("dag_sources", [])
    dag_types = payload.get("dag_types", [])
    dag_supplementary_files = payload.get("dag_supplementary_files", [])
    sub_time = payload.get("sub_time")

    if not (len(dag_ids) == len(dag_sources) == len(dag_types) == len(dag_supplementary_files)):
        return jsonify({"error": "Input lists (dag_ids, dag_sources, etc.) must have the same length."}), 400

    submitted_results = []
    # ä½¿ç”¨ zip è¿­ä»£ï¼Œå¤„ç†æ¯ä¸ªä»»åŠ¡
    for dag_id, source, dag_type, files in zip(dag_ids, dag_sources, dag_types, dag_supplementary_files):
        try:
            # 1. åŒæ­¥å‡†å¤‡ä»»åŠ¡æ•°æ®
            task_data_for_loader = {
                "dag_id": dag_id,
                "dag_source": source,
                "dag_type": dag_type,
                "supplementary_files": files
            }
            full_task_package = task_loader.load_task_package(task_data_for_loader, sub_time)

            dag_uuid = str(uuid.uuid4())
            final_package_to_queue = {
                "uuid": dag_uuid,
                "arrival_time": time.time(),
                "task_body": full_task_package
            }

            # 2. å°†å‡†å¤‡å¥½çš„ä»»åŠ¡æ”¾å…¥åå°é˜Ÿåˆ—
            # è¿™æ˜¯éé˜»å¡æ“ä½œï¼Œä¼šç«‹å³è¿”å›
            asyncio.run_coroutine_threadsafe(dag_queue.put(final_package_to_queue), loop)
            
            # 3. æ›´æ–°çŠ¶æ€å¹¶è®°å½•æˆåŠŸä¿¡æ¯
            dag_statuses[dag_uuid] = {"status": "queued", "submitted_at": time.time(), "dag_id": dag_id}
            submitted_results.append({"dag_id": dag_id, "uuid": dag_uuid})

        except Exception as e:
            import traceback
            print(f"âŒ Error submitting DAG '{dag_id}': {e}\n{traceback.format_exc()}")
            submitted_results.append({"dag_id": dag_id, "error": str(e)})
    
    # 4. è¿”å›å¤„ç†å›æ‰§ï¼Œé”®åä¸º "submitted" ä»¥åŒ¹é… dispatch_task.py
    return jsonify({"message": "DAG submission request processed.", "submitted": submitted_results}), 202

@app.route("/dag_status/<dag_uuid>", methods=["GET"])
def get_status_endpoint(dag_uuid: str):
    return jsonify(dag_statuses.get(dag_uuid, {"error": "DAG not found."}))

@app.route("/get_final_result/<dag_uuid>", methods=["GET"])
def get_final_result_endpoint(dag_uuid: str):
    status_info = dag_statuses.get(dag_uuid, {})
    if status_info.get("status") not in ["finished", "error"]:
        return jsonify({"status": status_info.get("status", "unknown"), "message": "DAG is still processing."})
    return jsonify(dag_results.get(dag_uuid, {"error": "Result not found for this DAG."}))


# --- åº”ç”¨å¯åŠ¨å’Œåå°å¾ªç¯è®¾ç½® (ä¸åŸæ–‡ä»¶ä¸€è‡´) ---
def run_asyncio_loop(loop_to_run):
    asyncio.set_event_loop(loop_to_run)
    loop_to_run.run_forever()

async def initialize_and_run_worker(queue, statuses, results, csv_logger, jsonl_logger, worker_args, endpoints_data):
    # æ­¤å¤„ä¼ é€’ vllm_endpoints_data ç»™ Worker
    worker = DAGWorker(queue, statuses, results, csv_logger, jsonl_logger, worker_args, endpoints_data)
    print("âœ… DAGWorker åˆå§‹åŒ–æˆåŠŸã€‚")
    asyncio.create_task(worker.consume_tasks())
    print("âœ… DAGWorker ä»»åŠ¡æ¶ˆè´¹è€…å·²åœ¨åå°å¯åŠ¨ã€‚")

if __name__ == "__main__":
    args = parse_host_arguments()
    ensure_log_files_exist()
    
    vllm_endpoints_data = {}
    try:
        with open(args.vllm_endpoints_json, 'r') as f:
            vllm_endpoints_data = json.load(f)
        print(f"âœ… VLLM é…ç½®æ–‡ä»¶ '{args.vllm_endpoints_json}' åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ— æ³•åŠ è½½VLLMé…ç½®æ–‡ä»¶ '{args.vllm_endpoints_json}'. é”™è¯¯: {e}")
        exit(1)
    
    task_loader = TaskLoader(args)
    
    dag_queue = asyncio.Queue()
    dag_statuses = {}
    dag_results = {}
    
    loop = asyncio.new_event_loop()
    loop_thread = threading.Thread(target=run_asyncio_loop, args=(loop,), daemon=True)
    loop_thread.start()
    print("ğŸŒ€ åå°Asyncioäº‹ä»¶å¾ªç¯å·²å¯åŠ¨ã€‚")

    # å°†è§£æå¥½çš„ vllm_endpoints_data ä¼ é€’ä¸‹å»
    asyncio.run_coroutine_threadsafe(
        initialize_and_run_worker(
            dag_queue, 
            dag_statuses, 
            dag_results, 
            log_to_csv, 
            log_to_jsonl, 
            args, 
            vllm_endpoints_data
        ),
        loop
    )

    print(f"ğŸš€ Flask HostæœåŠ¡å·²å¯åŠ¨ï¼Œç›‘å¬ http://0.0.0.0:{args.flask_port}")
    # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å»ºè®®ä½¿ç”¨ Gunicorn æˆ– uWSGI
    app.run(host='0.0.0.0', port=args.flask_port)