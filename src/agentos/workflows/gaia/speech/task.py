from agentos.scheduler import cpu,gpu,io
from pydub import AudioSegment
from dashscope import Generation
import dashscope
import oss2
from io import BytesIO
import ray
import os
import json
import gc
import torch
import numpy as np
import librosa
import time
import tempfile
import sys
import librosa
from pydub import AudioSegment # å¯¼å…¥pydub
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration
)
from typing import List, Dict

import re
def estimate_tokens(text):
    """
    ä¸€ä¸ªæ›´ç²¾ç¡®çš„ã€ç”¨äºä¼°ç®—ä¸­è‹±æ··åˆæ–‡æœ¬tokenæ•°çš„å‡½æ•°ã€‚
    å®ƒåˆ†åˆ«è®¡ç®—CJKå­—ç¬¦å’ŒéCJKå•è¯ï¼Œå¹¶é¿å…äº†é‡å¤è®¡ç®—ã€‚
    """
    # 1. ç²¾ç¡®è®¡ç®—CJKå­—ç¬¦æ•°
    cjk_chars = sum(1 for char in text if '\u4E00' <= char <= '\u9FFF')
    # 2. ä»æ–‡æœ¬ä¸­ç§»é™¤æ‰€æœ‰CJKå­—ç¬¦å’Œæ¢è¡Œç¬¦ï¼Œä»¥ä¾¿ç»Ÿè®¡å…¶ä»–è¯­è¨€çš„å•è¯
    # æˆ‘ä»¬ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†CJKå­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä»¥ç¡®ä¿å•è¯èƒ½è¢«æ­£ç¡®åˆ†å‰²
    non_cjk_text = re.sub(r'[\u4E00-\u9FFF]', ' ', text)
    non_cjk_text = non_cjk_text.replace("\n", " ")
    # 3. è®¡ç®—éCJKå•è¯æ•°ï¼Œä¹˜ä»¥ç»éªŒç³»æ•°
    # ä½¿ç”¨split()å¯ä»¥æœ‰æ•ˆåœ°æŒ‰ç©ºæ ¼åˆ†å‰²å‡ºå•è¯
    non_cjk_words_count = len(non_cjk_text.split())
    # 4. å°†ä¸¤éƒ¨åˆ†åŠ æ€»
    # è¿™é‡Œçš„1.3æ˜¯è‹±æ–‡å•è¯åˆ°tokençš„ç»éªŒç³»æ•°ï¼Œå¯ä»¥å¾®è°ƒ
    estimated_tokens = cjk_chars + int(non_cjk_words_count * 1.3)
    return estimated_tokens

def query_vllm_model(api_url: str, model_alias: str, messages: List, temperature: float= 0.6, max_token: int= 1024, top_p: float= 0.9, repetition_penalty: float= 1.1) -> tuple[dict, str]:
    """
    é€šè¿‡HTTPè¯·æ±‚æŸ¥è¯¢æœ¬åœ°vLLMæœåŠ¡ã€‚
    
    :param api_url: vLLMæœåŠ¡çš„æ ¹URL (ä¾‹å¦‚ http://127.0.0.1:8000)
    :param model_alias: åœ¨vLLMä¸­æœåŠ¡çš„æ¨¡å‹åˆ«å (ä¾‹å¦‚ qwen3-32b)
    :param messages: OpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
    :param temperature: æ¸©åº¦å‚æ•°
    :param max_token: æœ€å¤§ç”Ÿæˆtokenæ•°
    :param top_p: Top-pé‡‡æ ·å‚æ•°
    :param repetition_penalty: é‡å¤æƒ©ç½šå‚æ•°
    :return: ä¸€ä¸ªåŒ…å«æ€§èƒ½ç‰¹å¾å’Œæ¨¡å‹å“åº”æ–‡æœ¬çš„å…ƒç»„
    """
    import requests
    from rich.console import Console
    console = Console()
    # vLLMçš„èŠå¤©æ¥å£è·¯å¾„
    chat_url = f"{api_url.strip('/')}/v1/chat/completions"
    headers = {
        "Content-Type": "application/json"
    }
    # æ„å»ºä¸OpenAIå…¼å®¹çš„è¯·æ±‚ä½“
    payload = {
        "model": model_alias,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_token,
        "top_p": top_p,
        "repetition_penalty": repetition_penalty,
    }
    # ä¸ºäº†è®¡ç®—ç‰¹å¾ï¼Œå…ˆå°†æ¶ˆæ¯æ‹¼æ¥èµ·æ¥
    conversation = ""
    for message in messages:
        conversation += f"{message['role']}: {message['content']}"
    try:
        response = requests.post(chat_url, json=payload, headers=headers, timeout=3600)
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸
        
        response_data = response.json()
        content = response_data['choices'][0]['message']['content'].lstrip()
        
        # è¿”å›ä¸å…¶å®ƒqueryå‡½æ•°æ ¼å¼ä¸€è‡´çš„ç»“æœ
        features = {"text_length": len(conversation), "token_count": estimate_tokens(conversation)}
        return features, content

    except requests.exceptions.RequestException as e:
        error_msg = f"vLLM request failed: {str(e)}"
        console.print(f"[bold red]{error_msg}")
        features = {"text_length": len(conversation), "token_count": estimate_tokens(conversation)}
        return features, f"[bold red]{error_msg}"

def query_llm_online(api_url, api_key, payload: Dict[str, str], tokenizer_path: str)-> str:
    """Call SiliconFlow API to get LLM response"""
    import requests
    from rich.console import Console
    # å°†messagesè½¬æ¢æˆå­—ç¬¦ä¸²æ ¼å¼
    conversation= ""
    for message in payload["messages"]:
        conversation+= f"{message['role']}: {message['content']}"
        
    console= Console()
    headers= {
        "Authorization": api_key,
        "Content-Type": "application/json"
    }
    try:
        response= requests.post(
            api_url,
            json= payload,
            headers= headers,
            timeout= 3600
        )
        response.raise_for_status()
        return {"text_length": len(conversation), "token_count": estimate_tokens(conversation)}, response.json()['choices'][0]['message']['content'].lstrip()
    except Exception as e:
        console.print(f"[bold red]API request failed: {str(e)}")
        return {"text_length": len(conversation), "token_count": estimate_tokens(conversation)}, f"[bold red]API request failed: {str(e)}"

def query_llm(model:str, model_folder: str, messages:List, temperature:float= 0.6, max_token:int= 1024, top_p:float= 0.9, repetition_penalty:float= 1.1):
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer_path= os.path.join(model_folder, "Qwen/Qwen3-32B")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, device_map= "cuda")
    local_model = AutoModelForCausalLM.from_pretrained(
        os.path.join(model_folder, model),
        torch_dtype="float16",
        low_cpu_mem_usage=True,
        device_map="cuda", offload_state_dict= False,
        )

    # å°†messagesè½¬æ¢æˆå­—ç¬¦ä¸²æ ¼å¼
    conversation = ""
    for message in messages:
        conversation += f"{message['role']}: {message['content']}"

    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(conversation + tokenizer.eos_token, return_tensors='pt').to("cuda")

    # ç”Ÿæˆå›å¤
    output = local_model.generate(
        input_ids, 
        pad_token_id=tokenizer.eos_token_id,
        max_new_tokens= max_token,
        temperature= temperature, 
        top_p= top_p,
        repetition_penalty= repetition_penalty
    )
    response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)

    del local_model
    del tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    return {"text_length": len(conversation), "token_count": estimate_tokens(conversation)}, response

def decode_audio_to_array(audio_bytes: bytes, target_sr: int = 16000):
    """
    å¥å£®çš„éŸ³é¢‘è§£ç å‡½æ•°ï¼šå°†ä»»æ„æ ¼å¼çš„éŸ³é¢‘å­—èŠ‚è§£ç ä¸ºNumpyæ•°ç»„å’Œé‡‡æ ·ç‡ã€‚
    """
    print("æ­£åœ¨ä½¿ç”¨pydubè§£ç éŸ³é¢‘...")
    audio_segment = AudioSegment.from_file(BytesIO(audio_bytes))
    
    wav_buffer = BytesIO()
    audio_segment.export(wav_buffer, format="wav")
    wav_buffer.seek(0)
    print("è§£ç ä¸ºWAVæ ¼å¼æˆåŠŸã€‚æ­£åœ¨ä½¿ç”¨librosaåŠ è½½...")
    
    # librosaç°åœ¨å¯ä»¥ä»WAVæ ¼å¼çš„ç¼“å†²åŒºä¸­å®‰å…¨åœ°åŠ è½½æ•°æ®
    y, sr = librosa.load(wav_buffer, sr=target_sr)
    return y, sr

@cpu(cpu_num= 1, mem= 1024)
def task1_speech_process(context):
    """
    Reads read.jsonl, finds the task based on task_id, reads the corresponding audio file,
    and stores task info and file content in context.
    """
    try:
        start_time= time.time()
        supplementary_files = ray.get(context.get.remote("supplementary_files"))
        file_name = ""
        content = None
        
        if supplementary_files:
            file_name = next(iter(supplementary_files.keys()), "")
            content = next(iter(supplementary_files.values()), None)
            print(f"  -> Found single supplementary file to process: '{file_name}'")
        else:
            print("  -> No supplementary files found in context.")

        # è®¡ç®—éŸ³é¢‘ç‰¹å¾
        print("å¼€å§‹æå–éŸ³é¢‘ç‰¹å¾...")
        audio_array, sampling_rate = decode_audio_to_array(content, target_sr= 16000)
        print("æ­£åœ¨è®¡ç®—éŸ³é¢‘ç‰¹å¾...")
        S = np.abs(librosa.stft(audio_array))
        p = S / np.sum(S)
        audio_entropy = -np.sum(p * np.log2(p + 1e-10))
        audio_energy = np.sum(audio_array**2)
        duration = len(audio_array) / sampling_rate
        audio_features = {
            "duration": float(duration),
            "audio_entropy": float(audio_entropy),
            "audio_energy": float(audio_energy)
        }
        print(f"éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {audio_features}")

        # æ„å»ºç‰¹å¾JSON
        audio_features= {
            # åŸºæœ¬éŸ³é¢‘ç‰¹å¾
            "duration": audio_features["duration"],
            "audio_entropy": audio_features["audio_entropy"],
            "audio_energy": audio_features["audio_energy"]
        }
        
        # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯å’Œæ–‡ä»¶å†…å®¹
        context.put.remote("file_content", audio_array)
        context.put.remote("audio_features", audio_features)
        print(f"éŸ³é¢‘ç‰¹å¾æå–å®Œæˆ: {audio_features}")
        return json.dumps({
            "succ_task_feat": audio_features,
            "start_time": start_time,
            "end_time": time.time()
        })

    except Exception as e:
        print(f"task1 å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise e

@gpu(gpu_mem= 15000)
def task2_speech_process(context):
    """
    Processes the audio file using Whisper for speech recognition.
    """
    try:
        start_time = time.time()
        dag_id= ray.get(context.get.remote('dag_id'))
        question = ray.get(context.get.remote("question"))
        content = ray.get(context.get.remote("file_content"))
        model_folder= ray.get(context.get.remote("model_folder"))

        if not question:
             raise ValueError(f"ä»»åŠ¡ {dag_id} ç¼ºå°‘ Question")

        print("åŠ è½½ Whisper æ¨¡å‹ä¸­...")
        model_path= os.path.join(model_folder, "whisper_models/whisper-medium")
        processor= WhisperProcessor.from_pretrained(model_path)
        model= WhisperForConditionalGeneration.from_pretrained(model_path)
        model.generation_config.forced_decoder_ids= None
        print("å·²ä»ä¸Šä¸‹æ–‡ä¸­è·å–è§£ç åçš„éŸ³é¢‘æ•°ç»„ã€‚")

        # ä½¿ç”¨è·å–åˆ°çš„æ•°ç»„å’Œé‡‡æ ·ç‡è¿›è¡Œå¤„ç†
        input_features = processor(
            content, # ä½¿ç”¨æœ¬åœ°åŠ è½½çš„éŸ³é¢‘æ•°ç»„
            sampling_rate=16000, # ä½¿ç”¨æœ¬åœ°åŠ è½½çš„é‡‡æ ·ç‡
            return_tensors="pt"
        ).input_features

        predicted_ids = model.generate(input_features)
        processed_content = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        print("ğŸ”Š Whisper è¯†åˆ«ç»“æœï¼š", processed_content)
        
        context.put.remote("processed_content", processed_content)
        if not question:
             raise ValueError(f"ä»»åŠ¡ {dag_id} ç¼ºå°‘ Question")
        # ç”Ÿæˆæ¨ç†æ¨¡å‹çš„ç‰¹å¾
        prompt= (
            "#Background#\n"
            "You are a general AI assistant. I will ask you a question. Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\n"
            "YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.\n"
            "If you are asked for a number, donâ€™t use comma to write your number neither use units such as $ or percent sign unless specified otherwise.\n"
            "If you are asked for a string, donâ€™t use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.\n"
            "If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\n"
            f"#Question#\n{question}\n"
            "#Extracted text from speech#\n"
            f"{processed_content[:8000]}\n"
        )
        succ_task_feat= {"text_length": len(prompt), "token_count": estimate_tokens(prompt)}
        audio_features= ray.get(context.get.remote("audio_features"))
        return json.dumps({ # ä¸ºäº†ä¿å­˜åˆ°æœ¬åœ°
            "succ_task_feat": {
                "task3_llm_process_qwen": {"text_length": succ_task_feat["text_length"], "token_count": succ_task_feat["token_count"], "reason": 1},
                "task4_llm_process_deepseek": {"text_length": succ_task_feat["text_length"], "token_count": succ_task_feat["token_count"], "reason": 0}
            },
            "dag_id": dag_id,
            "curr_task_feat": audio_features,
            "start_time": start_time,
            "end_time": time.time()
        })
    except Exception as e:
        print(f"task2_speech_recognition å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        print(f"task2_speech_recognition é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise e

@gpu(gpu_mem= 70000, model_name= "qwen3-32b", backend="huggingface")
def task3_llm_process_qwen(context):
    
    """
    Processes the file content using LLM based on the question.
    """
    try:
        backend= task3_llm_process_qwen._task_decorator["backend"]
        print(f"âœ… qwenå¤„ç†å¼€å§‹....")
        start_time= time.time()  # ADD: è®°å½•å¼€å§‹æ—¶é—´ 
        dag_id= ray.get(context.get.remote("dag_id"))
        processed_content = ray.get(context.get.remote("processed_content")) 
        question= ray.get(context.get.remote("question"))

        use_online_model= ray.get(context.get.remote("use_online_model"))
        model_folder= ray.get(context.get.remote("model_folder"))
        tokenizer_path= os.path.join(model_folder, "Qwen/Qwen3-32B")
        temperature, max_token, repetition_penalty, top_p= None, None, None, None
        if not use_online_model:
            temperature= ray.get(context.get.remote("temperature"))
            max_token= ray.get(context.get.remote("max_tokens"))
            top_p= ray.get(context.get.remote("top_p"))
            repetition_penalty= ray.get(context.get.remote("repetition_penalty"))
        if not question:
             raise ValueError(f"ä»»åŠ¡ {dag_id} ç¼ºå°‘ Question")

        # æ„å»ºæç¤º
        # ç”Ÿæˆæ¨ç†æ¨¡å‹çš„ç‰¹å¾
        prompt= (
            "#Background#\n"
            "You are a general AI assistant. I will ask you a question. Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\n"
            "YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.\n"
            "If you are asked for a number, donâ€™t use comma to write your number neither use units such as $ or percent sign unless specified otherwise.\n"
            "If you are asked for a string, donâ€™t use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.\n"
            "If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\n"
            f"#Question#\n{question}\n"
            "#Extracted text from file#\n"
            f"{processed_content[:8000]}\n"
        )
        payload= {
            "model": "Qwen/Qwen3-32B",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_token,
            "enable_thinking": False,
            "response_format": {"type": "text"}               
        }
        inference_features, answer= None, None
        if use_online_model:
            inference_features, answer= query_llm_online(
                api_url= ray.get(context.get.remote("api_url")), 
                api_key= ray.get(context.get.remote("api_key")), 
                payload= payload,
                tokenizer_path= tokenizer_path)
        elif backend == "vllm":
            inference_features, answer= query_vllm_model(
                api_url= ray.get(context.get.remote("task3_llm_process_qwen_request_api_url")),
                model_alias= "qwen3-32b",
                messages= payload["messages"],
                temperature= temperature,
                max_token= max_token,
                top_p= top_p,
                repetition_penalty= repetition_penalty)
        else:
            inference_features, answer= query_llm(model_folder= model_folder, model= "Qwen/Qwen3-32B", messages= [{"role": "user", "content": prompt}], temperature= temperature, max_token= max_token, top_p= top_p, repetition_penalty= repetition_penalty)
        context.put.remote("qwen_answer", answer)
        inference_features['reason'] = 0
        next_task_prompt = (
            "You are a senior editor and a world-class reasoning expert. Your job is to synthesize the answers from two different AI assistants to produce one final, superior answer for the given question.\n\n"
            f"--- Original Question ---\n{question}\n\n"
            f"--- Answer from Assistant 1 (Qwen3) ---"
            f"--- Answer from Assistant 2 (DeepSeek) ---"
            "--- Your Task ---\n"
            "Analyze both answers. Identify the strengths and weaknesses of each. Then, combine their best elements, correct any errors, and provide a single, comprehensive, and accurate final answer. Adhere to the final answer format requested in the original prompt.\n\n"
            "Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER]."
        )
        text1_length= len(answer)
        text1_token_count= estimate_tokens(answer)
        context.put.remote("text1_feature", {"text1_length": text1_length, "text1_token_count": text1_token_count})
        return json.dumps({
            "dag_id": dag_id,
            "curr_task_feat": inference_features,
            "succ_task_feat": {
                "task4_llm_fuse_answer":{
                    "prompt_length": len(next_task_prompt),
                    "prompt_token_count": estimate_tokens(next_task_prompt),
                    "text1_length": text1_length,
                    "text1_token_count": text1_token_count,
                    "reason": 0
                }
            },
            "start_time": start_time,
            "end_time": time.time()
        })
    except Exception as e:
        print(f"task3_llm_process_qwen å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise e

@gpu(gpu_mem= 70000, model_name= "deepseek-r1-32b", backend="huggingface")
def task4_llm_process_deepseek(context):
    
    """
    Processes the file content using LLM based on the question.
    """
    try:
        backend= task4_llm_process_deepseek._task_decorator["backend"]
        start_time= time.time()  # ADD: è®°å½•å¼€å§‹æ—¶é—´ 
        print(f"âœ… deepseekå¤„ç†å¼€å§‹....")
        dag_id= ray.get(context.get.remote("dag_id"))
        processed_content = ray.get(context.get.remote("processed_content")) 
        question= ray.get(context.get.remote("question"))

        use_online_model= ray.get(context.get.remote("use_online_model"))
        model_folder= ray.get(context.get.remote("model_folder"))
        tokenizer_path= os.path.join(model_folder, "Qwen/Qwen3-32B")
        temperature, max_token, repetition_penalty, top_p= None, None, None, None
        if not use_online_model:
            temperature= ray.get(context.get.remote("temperature"))
            max_token= ray.get(context.get.remote("max_tokens"))
            top_p= ray.get(context.get.remote("top_p"))
            repetition_penalty= ray.get(context.get.remote("repetition_penalty"))
        if not question:
             raise ValueError(f"ä»»åŠ¡ {dag_id} ç¼ºå°‘ Question")
        # æ„å»ºæç¤º
        # ç”Ÿæˆæ¨ç†æ¨¡å‹çš„ç‰¹å¾
        prompt= (
            "#Background#\n"
            "You are a general AI assistant. I will ask you a question. Report your concise thinking thoughts and don't think too complicated, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\n"
            "YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.\n"
            "If you are asked for a number, donâ€™t use comma to write your number neither use units such as $ or percent sign unless specified otherwise.\n"
            "If you are asked for a string, donâ€™t use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.\n"
            "If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\n"
            f"#Question#\n{question}\n"
            "#Extracted text from file#\n"
            f"{processed_content[:8000]}\n"
        )
        payload= {
            "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_token,
            "response_format": {"type": "text"}
        }
        inference_features, answer= None, None
        if use_online_model:
            inference_features, answer= query_llm_online(
                api_url= ray.get(context.get.remote("api_url")), 
                api_key= ray.get(context.get.remote("api_key")), 
                payload= payload,
                tokenizer_path= tokenizer_path)
        elif backend == "vllm":
            inference_features, answer= query_vllm_model(
                api_url= ray.get(context.get.remote("task4_llm_process_deepseek_request_api_url")),
                model_alias= "deepseek-r1-32b",
                messages= payload["messages"],
                temperature= temperature,
                max_token= max_token,
                top_p= top_p,
                repetition_penalty= repetition_penalty)
        else:
            inference_features, answer= query_llm(model_folder= model_folder, model= "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", messages= [{"role": "user", "content": prompt}], temperature= temperature, max_token= max_token, top_p= top_p, repetition_penalty= repetition_penalty)        
        context.put.remote("deepseek_answer", answer)
        inference_features['reason'] = 1        
        next_task_prompt = (
            "You are a senior editor and a world-class reasoning expert. Your job is to synthesize the answers from two different AI assistants to produce one final, superior answer for the given question.\n\n"
            f"--- Original Question ---\n{question}\n\n"
            f"--- Answer from Assistant 1 (Qwen3) ---"
            f"--- Answer from Assistant 2 (DeepSeek) ---"
            "--- Your Task ---\n"
            "Analyze both answers. Identify the strengths and weaknesses of each. Then, combine their best elements, correct any errors, and provide a single, comprehensive, and accurate final answer. Adhere to the final answer format requested in the original prompt.\n\n"
            "Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER]."
        )
        text2_length= len(answer)
        text2_token_count= estimate_tokens(answer)
        context.put.remote("text2_feature", {"text2_length": text2_length, "text2_token_count": text2_token_count})
        return json.dumps({
            "dag_id": dag_id,
            "curr_task_feat": inference_features,
            "succ_task_feat": {
                "task4_llm_fuse_answer":{
                    "prompt_length": len(next_task_prompt),
                    "prompt_token_count": estimate_tokens(next_task_prompt),         
                    "text2_length": text2_length,
                    "text2_token_count": text2_token_count,
                    "reason": 0
                }
            },            
            "start_time": start_time,
            "end_time": time.time()
        })
    except Exception as e:
        print(f"task4_llm_process_deepseek å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise e

@gpu(gpu_mem= 70000, model_name= "qwen3-32b", backend="huggingface") # èåˆä»»åŠ¡é€šå¸¸ä¹Ÿéœ€è¦LLMï¼Œä½†å¯èƒ½æ¯”ç”Ÿæˆä»»åŠ¡èµ„æºæ¶ˆè€—å°
def task5_llm_fuse_answer(context):
    """
    Fusion Task: Takes answers from multiple experts, analyzes them, and generates a final, synthesized answer.
    """
    try:
        backend= task5_llm_fuse_answer._task_decorator["backend"]
        print("âœ… Task 4 (Fusion): Starting answer synthesis...")
        start_time= time.time()  # ADD: è®°å½•å¼€å§‹æ—¶é—´ 
        # 1. ä» context è·å–æ‰€æœ‰ä¸“å®¶çš„ç­”æ¡ˆ
        answer_qwen = ray.get(context.get.remote("qwen_answer"))
        answer_deepseek = ray.get(context.get.remote("deepseek_answer"))
        dag_id = ray.get(context.get.remote("dag_id"))
        question = ray.get(context.get.remote("question"))
        text1_feature= ray.get(context.get.remote("text1_feature"))
        text2_feature= ray.get(context.get.remote("text2_feature"))
        use_online_model= ray.get(context.get.remote("use_online_model"))
        model_folder= ray.get(context.get.remote("model_folder"))
        tokenizer_path= os.path.join(model_folder, "Qwen/Qwen3-32B")
        temperature, max_token, repetition_penalty, top_p= None, None, None, None
        if not use_online_model:
            temperature= ray.get(context.get.remote("temperature"))
            max_token= ray.get(context.get.remote("max_tokens"))
            top_p= ray.get(context.get.remote("top_p"))
            repetition_penalty= ray.get(context.get.remote("repetition_penalty"))
        if not question:
             raise ValueError(f"ä»»åŠ¡ {dag_id} ç¼ºå°‘ Question")

        # 2. æ„å»ºä¸€ä¸ªç”¨äºèåˆçš„ prompt
        prompt = (
            "You are a senior editor and a world-class reasoning expert. Your job is to synthesize the answers from two different AI assistants to produce one final, superior answer for the given question.\n\n"
            f"--- Original Question ---\n{question}\n\n"
            f"--- Answer from Assistant 1 (Qwen3) ---\n{answer_qwen}\n\n"
            f"--- Answer from Assistant 2 (DeepSeek) ---\n{answer_deepseek}\n\n"
            "--- Your Task ---\n"
            "Analyze both answers. Identify the strengths and weaknesses of each. Then, combine their best elements, correct any errors, and provide a single, comprehensive, and accurate final answer. Adhere to the final answer format requested in the original prompt.\n\n"
            "Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER]."
        )
        
        # 3. ä½¿ç”¨ä¸€ä¸ªå¼ºå¤§çš„æ¨¡å‹ï¼ˆå¦‚Qwen3ï¼‰æ¥æ‰§è¡Œèåˆ
        payload= {
            "model": "Qwen/Qwen3-32B",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_token,
            "enable_thinking": False,
            "response_format": {"type": "text"}               
        }
        final_answer= None
        if use_online_model:
            inference_features, final_answer= query_llm_online(
                api_url= ray.get(context.get.remote("api_url")), 
                api_key= ray.get(context.get.remote("api_key")), 
                payload= payload,
                tokenizer_path= tokenizer_path)
        elif backend == "vllm":
            inference_features, final_answer= query_vllm_model(
                api_url= ray.get(context.get.remote("task5_llm_fuse_answer_request_api_url")),
                model_alias= "qwen3-32b",
                messages= payload["messages"],
                temperature= temperature,
                max_token= max_token,
                top_p= top_p,
                repetition_penalty= repetition_penalty)
        else:
            inference_features, final_answer= query_llm(model_folder= model_folder, model= "Qwen/Qwen3-32B", messages= [{"role": "user", "content": prompt}], temperature= temperature, max_token= max_token, top_p= top_p, repetition_penalty= repetition_penalty)        
        # 4. å‡†å¤‡å¹¶è¿”å›æœ€ç»ˆçš„è¾“å‡º
        print(f"  -> Fusion complete. Final answer generated for DAG {dag_id}.")
        return json.dumps({
            "dag_id": dag_id,
            "final_answer": final_answer,
            "curr_task_feat": 
            {
                "prompt_length": inference_features["text_length"], 
                "prompt_token_count": inference_features["token_count"], 
                "text1_length": text1_feature["text1_length"],
                "text1_token_count": text1_feature["text1_token_count"],
                "text2_length": text2_feature["text2_length"],
                "text2_token_count": text2_feature["text2_token_count"],
                "reason": 0
            },
            "start_time": start_time,
            "end_time": time.time()
        })
    except Exception as e:
        print(f"âŒ task5_llm_fuse_answer failed: {str(e)}")
        raise e