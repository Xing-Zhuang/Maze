import queue
import threading
import time
import requests
import networkx as nx
import cloudpickle
import importlib.util
import json
import redis
import traceback

from agentos.utils.scheduler_algorithm_utils import compute_ranku, TASK_TYPE_DEFAULT_EXEC_TIMES
from agentos.utils.query_loader import GaiaLoader, TBenchLoader,OpenAGILoader

query_loader_factory = {
    "gaia": GaiaLoader,
    "tbench": TBenchLoader,
    "openagi": OpenAGILoader
}

def dag_manager_heft(args, dag_que, dag_status_dict):
    """
    ä¸€ä¸ªä¸¥æ ¼éµå¾ªåŸå§‹è®ºæ–‡æ€æƒ³çš„HEFTç®—æ³•åŠ¨æ€è°ƒåº¦ç®¡ç†å™¨ã€‚
    - æ ¸å¿ƒæ€æƒ³: 
        1. ä½¿ç”¨rankuå€¼å¯¹ä»»åŠ¡è¿›è¡Œä¼˜å…ˆçº§æ’åºã€‚
        2. å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œéå†æ‰€æœ‰è®¡ç®—èŠ‚ç‚¹ï¼Œæ‰¾åˆ°èƒ½ä½¿å…¶æœ€æ—©å®Œæˆ(EFT)çš„æ—¶é—´ç‚¹å’Œå¯¹åº”çš„æœ€æ—©å¼€å§‹æ—¶é—´(EST)ã€‚
        3. å°†ä»»åŠ¡è°ƒåº¦åˆ°æœ€ä¼˜èŠ‚ç‚¹ã€‚
    - éµå¾ªåŸå§‹Paper:
        - æœ¬å®ç°é€šè¿‡æ”¹é€ submitterçº¿ç¨‹ï¼Œç¡®ä¿ä»»åŠ¡åªæœ‰åœ¨â€œçœŸå®æ—¶é—´â€è¾¾åˆ°å…¶â€œé¢„ä¼°å¼€å§‹æ—¶é—´(EST)â€ä¹‹åæ‰ä¼šè¢«æ´¾å‘ã€‚
        - è¿™ç§æ–¹å¼ä¸¥æ ¼æ¨¡æ‹Ÿäº†é™æ€è°ƒåº¦ä¸­çš„æ—¶é—´è¡¨ï¼Œé€‚ç”¨äºç§‘ç ”å’Œç®—æ³•éªŒè¯åœºæ™¯ã€‚
    """
    
    # --- çŠ¶æ€ç»´æŠ¤ ---
    dags_data = {}
    
    try:
        res = requests.post(f"http://{args.master_addr}/resource")
        res.raise_for_status()
        available_nodes = list(res.json().keys())
        if not available_nodes:
            raise ValueError("No available compute nodes found from master.")
        print(f"âœ… HEFT Manager: Successfully discovered {len(available_nodes)} nodes: {available_nodes}")
    except Exception as e:
        print(f"âŒ [FATAL] Failed to get available nodes from master: {e}. Exiting.")
        return

    proc_schedules = {node: [] for node in available_nodes}
    task_to_node_map = {} 
    
    # å°±ç»ªä»»åŠ¡åˆ—è¡¨ (ä»»åŠ¡çš„æ•°æ®ä¾èµ–å·²æ»¡è¶³)
    ready_list = []
    ready_list_lock = threading.Lock()
    
    monitor_que = queue.Queue()
    
    exec_time_db = {}
    redis_client = redis.Redis(host=args.redis_ip, port=args.redis_port, decode_responses=False)
    print("ğŸ’  HEFT manager (Strict Paper Version) initialized.")


    def find_insert_time_slot(schedule_list, ready_time, exec_time):
        """
        åœ¨èŠ‚ç‚¹çš„è°ƒåº¦åˆ—è¡¨ä¸Šä¸ºä»»åŠ¡æ‰¾åˆ°æœ€æ—©çš„å¯ç”¨æ—¶é—´æ’æ§½ã€‚
        (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œé€»è¾‘ä¸åŸæ–‡ä¸€è‡´)
        """
        if not schedule_list:
            return ready_time

        if ready_time + exec_time <= schedule_list[0][0]:
            return ready_time

        for i in range(len(schedule_list) - 1):
            prev_end = schedule_list[i][1]
            next_start = schedule_list[i + 1][0]
            start_candidate = max(ready_time, prev_end)
            if start_candidate + exec_time <= next_start:
                return start_candidate

        return max(ready_time, schedule_list[-1][1])

    def schedule_dag(run_id):
        """
        å¯¹å•ä¸ªDAGæ‰§è¡Œå®Œæ•´çš„HEFTè°ƒåº¦ç®—æ³•ï¼Œç”Ÿæˆè°ƒåº¦è®¡åˆ’å¹¶å¡«å……åˆå§‹å°±ç»ªåˆ—è¡¨ã€‚
        (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹ï¼Œé€»è¾‘ä¸åŸæ–‡ä¸€è‡´)
        """
        dag_graph = dags_data[run_id]
        dag_id= dag_graph.graph["dag_id"]
        ranku_values = compute_ranku(dag_graph, exec_time_db, run_id)
        
        task_list = sorted(dag_graph.nodes(), key=lambda task: ranku_values.get(task, 0), reverse=True)
        
        print(f"ğŸ’  HEFT Scheduler: Planning for DAG '{dag_id}'. Task order: {task_list}")

        for task_name in task_list:
            ready_time = 0.0
            for pred in dag_graph.predecessors(task_name):
                pred_info = dag_graph.nodes[pred]
                ready_time = max(ready_time, pred_info.get('eft', 0.0))

            exec_time = exec_time_db.get((run_id, task_name))
            if exec_time is None:
                task_type = dag_graph.nodes[task_name].get('type')
                exec_time = TASK_TYPE_DEFAULT_EXEC_TIMES.get(task_type, TASK_TYPE_DEFAULT_EXEC_TIMES['default'])

            best_node, best_eft, best_est = None, float('inf'), 0.0

            for node_id in proc_schedules.keys():
                schedule_on_node = proc_schedules[node_id]
                est = find_insert_time_slot(schedule_on_node, ready_time, exec_time)
                eft = est + exec_time
                
                if eft < best_eft:
                    best_eft, best_est, best_node = eft, est, node_id
            
            print(f"  -> Task '{task_name}' scheduled on node '{best_node}' at EST: {best_est:.2f}, finishes at EFT: {best_eft:.2f}")
            dag_graph.nodes[task_name]['est'] = best_est
            dag_graph.nodes[task_name]['eft'] = best_eft
            dag_graph.nodes[task_name]['node'] = best_node
            
            proc_schedules[best_node].append((best_est, best_eft, (run_id, task_name)))
            proc_schedules[best_node].sort(key=lambda x: x[0])
            task_to_node_map[(run_id, task_name)] = best_node

        with ready_list_lock:
            for node in dag_graph.nodes():
                if dag_graph.in_degree(node) == 0:
                    ready_list.append((run_id, node))
        print(f"  -> Initial ready tasks for DAG '{dag_id}' have been queued for submission.")

    def dag_creator():
        """
        çº¿ç¨‹å‡½æ•°ï¼šæ¥æ”¶æ–°DAGï¼Œä¸ºå…¶ç”Ÿæˆè°ƒåº¦è®¡åˆ’ï¼Œå¹¶è®°å½•è°ƒåº¦å¼€å§‹æ—¶é—´ã€‚
        """
        while True:
            run_id, dag_id, dag_source, dag_type, supplementary_files, task2id, sub_time = dag_que.get()
            print(f"ğŸ’  HEFT Creator: Received new DAG '{dag_id}'")

            try:
                query_loader = query_loader_factory.get(dag_source)
                loader = query_loader(args= args, dag_id= dag_id, run_id= run_id, dag_type= dag_type, dag_source= dag_source, supplementary_files= supplementary_files, sub_time= sub_time)
                dag_graph = loader.get_dag(task2id)
                dags_data[run_id] = dag_graph
                
                # --- MODIFICATION START ---
                # è®°å½•è°ƒåº¦è¿‡ç¨‹çš„â€œ0æ—¶åˆ»â€ï¼Œç”¨äºåç»­è®¡ç®—çœŸå®ç­‰å¾…æ—¶é—´
                dag_graph.graph['schedule_start_time'] = time.time()
                print(f"  -> Set schedule start time for DAG '{dag_id}' to {dag_graph.graph['schedule_start_time']:.2f}")
                # --- MODIFICATION END ---
                schedule_dag(run_id)
                time.sleep(0.01)
            except Exception as e:
                print(f"âŒ [Error] Failed during DAG creation/scheduling for '{dag_id}': {e}")
                print(traceback.format_exc())

    def submitter():
        """
        çº¿ç¨‹å‡½æ•°ï¼šä»å°±ç»ªåˆ—è¡¨ä¸­å–å‡ºä»»åŠ¡ï¼Œæ£€æŸ¥æ˜¯å¦åˆ°è¾¾å…¶ESTï¼Œç„¶åæäº¤ã€‚
        """
        # --- MODIFICATION START ---
        # é‡å†™æ•´ä¸ªsubmitterçš„é€»è¾‘
        task_order= 1
        while True:
            task_to_submit = None
            
            with ready_list_lock:
                # éå†å°±ç»ªåˆ—è¡¨ï¼Œå¯»æ‰¾å¯ä»¥æäº¤çš„ä»»åŠ¡
                for run_id, func_name in ready_list:
                    dag_graph = dags_data[run_id]
                    task_node = dag_graph.nodes[func_name]
                    
                    # è·å–é¢„ä¼°çš„å¼€å§‹æ—¶é—´(EST)å’Œè°ƒåº¦å‚è€ƒçš„0æ—¶åˆ»
                    task_est = task_node.get('est', 0.0)
                    schedule_start_time = dag_graph.graph.get('schedule_start_time', time.time())
                    
                    # è®¡ç®—ä»è°ƒåº¦å¼€å§‹åˆ°å½“å‰è¿‡å»äº†å¤šå°‘çœŸå®æ—¶é—´
                    current_elapsed_time = time.time() - schedule_start_time
                    
                    # åªæœ‰å½“çœŸå®æµé€æ—¶é—´ >= é¢„ä¼°å¼€å§‹æ—¶é—´ï¼Œä»»åŠ¡æ‰è¢«æ´¾å‘
                    if current_elapsed_time >= task_est:
                        task_to_submit = (run_id, func_name)
                        print(f"ğŸ HEFT Submitter: Task '{func_name}' EST of {task_est:.2f} has been reached (current elapsed: {current_elapsed_time:.2f}). Picking for submission.")
                        ready_list.remove(task_to_submit)
                        break # ä¸€æ¬¡åªæäº¤ä¸€ä¸ªï¼Œä¿æŒå¾ªç¯ç®€å•
            
            if task_to_submit:
                run_id, func_name = task_to_submit
                dag_graph = dags_data[run_id]
                task_info = dict(dag_graph.nodes[func_name])
                
                task_info['node_id'] = task_to_node_map.get((run_id, func_name))
                if not task_info.get('node_id'):
                    print(f"âŒ [FATAL] Could not find scheduled node for task '{func_name}'.")
                    continue

                task_info["run_id"] = run_id
                task_info["dag_id"] = dag_graph.graph["dag_id"]
                task_info["question"] = dag_graph.graph.get("question", "")
                task_info["answer"] = dag_graph.graph.get("answer", "")
                task_info["supplementary_file_paths"] = dag_graph.graph.get("supplementary_file_paths", {})
                task_info["dag_func_file"] = dag_graph.graph.get("dag_func_file", "")
                task_info["arrival_time"] = dag_graph.graph.get("arrival_time", time.time())
                task_info["priority"]= task_order
                task_order+= 1
                try:
                    spec = importlib.util.spec_from_file_location("dag_module", task_info["dag_func_file"])
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    func = getattr(module, func_name)
                    serialized_func = cloudpickle.dumps(func)
                    
                    redis_client.set(f"func:{task_info['task_id']}", serialized_func)
                    
                    print(f"  -> Submitting task '{func_name}' to the master scheduler, AFFINITY to node '{task_info['node_id']}'.")
                    requests.post(f"http://{args.master_addr}/inform", json=task_info)
                    monitor_que.put((run_id, func_name))
                except Exception as e:
                    print(f"âŒ [Error] Failed to submit task '{func_name}': {e}")
            # å¦‚æœæ²¡æœ‰ä»»åŠ¡åˆ°è¾¾ESTï¼ŒçŸ­æš‚ä¼‘çœ ï¼Œé¿å…CPUç©ºè½¬
            time.sleep(0.01)
        # --- MODIFICATION END ---

    def monitor():
        """
        çº¿ç¨‹å‡½æ•°ï¼šé€šè¿‡ç›‘å¬Redisæ¶ˆæ¯é˜Ÿåˆ—æ¥é«˜æ•ˆåœ°è·å–ä»»åŠ¡å®Œæˆé€šçŸ¥ï¼Œ
        å¹¶è§¦å‘åç»­ä¾èµ–ä»»åŠ¡ã€‚
        """
        # --- MODIFICATION START ---
        # This thread is completely rewritten for event-driven notifications.
        
        # 1. Create a Redis client for this specific thread
        redis_client = redis.Redis(host=args.redis_ip, port=args.redis_port, decode_responses=True)
        completion_queue_name = "task_completion_queue"
        print(f"ğŸ’  HEFT Monitor is now listening on Redis queue: '{completion_queue_name}'")

        while True:
            try:
                # 2. Efficiently block and wait for a message. No more polling.
                # The timeout is a fail-safe, it will wait indefinitely if set to 0.
                message = redis_client.brpop(completion_queue_name, timeout=0)
                if not message:
                    continue

                # message is a tuple (queue_name, data), we need the data part.
                notification = json.loads(message[1])
                dag_id = notification["dag_id"]
                run_id= notification["run_id"]
                task_id = notification["task_id"]
                func_name = notification["func_name"]
                status = notification["status"]
                
                print(f"âœ… HEFT Monitor (Event-Driven): Received completion for task '{func_name}' with status '{status}'.")

                dag_graph = dags_data.get(run_id)
                if not dag_graph:
                    print(f"âš ï¸ Warning: Received notification for an unknown DAG ID '{dag_id}'. Skipping.")
                    continue
                
                # 3. Update the detailed execution stats in the shared dictionary.
                # This logic is still useful for the final /release call.
                try:
                    task_result_raw = redis_client.get(f"result:{task_id}") # The runner still stores detailed results here
                    task_result = json.loads(task_result_raw) if task_result_raw else {}
                    
                    start_exec_time = task_result.get("start_time", 0.0)
                    finish_exec_time = task_result.get("end_time", 0.0)
                    arrival_time = dag_graph.graph.get("arrival_time", 0.0)
                    sub_time= dag_graph.graph.get("sub_time", 0.0)
                    leave_time = time.time()

                    current_dag_status = dict(dag_status_dict[run_id])
                    task_status_info = current_dag_status.get(func_name, {})
                    task_status_info['status'] = status
                    task_status_info['sub_time']= sub_time
                    task_status_info['start_exec_time'] = start_exec_time
                    task_status_info['finish_exec_time'] = finish_exec_time
                    task_status_info['arrival_time'] = arrival_time
                    task_status_info['leave_time'] = leave_time

                    current_dag_status[func_name] = task_status_info
                    dag_status_dict[run_id] = current_dag_status
                except Exception as e:
                    print(f"âŒ [Error] Failed to update dag_status_dict for '{func_name}': {e}")
                    # Even if stats update fails, we must mark status to unblock dependents
                    current_dag_status = dict(dag_status_dict[run_id])
                    current_dag_status.setdefault(func_name, {})['status'] = status
                    dag_status_dict[run_id] = current_dag_status

                if status != "finished":
                    continue

                # 4. Trigger dependent tasks (the core logic remains the same)
                dag_graph.nodes[func_name]['status'] = 'finished'
                for successor in dag_graph.successors(func_name):
                    all_preds_done = all(
                        dag_graph.nodes[pred].get('status') == 'finished' 
                        for pred in dag_graph.predecessors(successor)
                    )
                    
                    if all_preds_done:
                        with ready_list_lock:
                            if (run_id, successor) not in ready_list:
                                print(f"  -> Dependency met for '{successor}'. Added to ready list.")
                                ready_list.append((run_id, successor))
                time.sleep(0.01)
            except Exception as e:
                print(f"âŒ [FATAL] An error occurred in the HEFT monitor thread: {e}")
                print(traceback.format_exc())
                time.sleep(5) # Avoid rapid-fire errors
    
    # --- ä¸»å‡½æ•°é€»è¾‘ ---
    print("ğŸš€ Starting HEFT scheduler manager (Strict Paper Version)...")
    creator_thread = threading.Thread(target=dag_creator, daemon=True)
    submitter_thread = threading.Thread(target=submitter, daemon=True)
    monitor_thread = threading.Thread(target=monitor, daemon=True)
    
    creator_thread.start()
    submitter_thread.start()
    monitor_thread.start()
    
    print("âœ… HEFT scheduler manager and its worker threads are running.")
    creator_thread.join()