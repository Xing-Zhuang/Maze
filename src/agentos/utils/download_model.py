import os
import easyocr
import time
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"  # å¯ç”¨é«˜é€Ÿä¸‹è½½
os.environ["HF_HUB_OFFLINE"] = "0"  # ç¡®ä¿ä¸ä½¿ç”¨ç¦»çº¿æ¨¡å¼
from pathlib import Path
from typing import List
import requests # æ–°å¢ï¼šç”¨äºä¸‹è½½æµ‹è¯•å›¾ç‰‡
from PIL import Image # æ–°å¢ï¼šç”¨äºå¤„ç†å›¾ç‰‡
from io import BytesIO # æ–°å¢ï¼šç”¨äºå¤„ç†å›¾ç‰‡æ•°æ®æµ
import torch
from transformers import pipeline
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from datasets import load_dataset
import transformers.utils.hub as hub
hub.HF_ENDPOINT = "https://hf-mirror.com"
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoProcessor,
    AutoModelForImageTextToText,
    WhisperProcessor,
    WhisperForConditionalGeneration,
    BlipProcessor,
    BlipForConditionalGeneration
)

# è®¾ç½®å¯è§çš„gpu
# os.environ["CUDA_VISIBLE_DEVICES"] = "1,2,3"
def test_sentiment_analysis_inference(model_path: Path) -> bool:
    """æµ‹è¯•å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¨¡å‹çš„æ¨ç†åŠŸèƒ½ã€‚"""
    try:
        # åŠ è½½æ¨¡å‹
        sentiment_analyzer = pipeline(
            "sentiment-analysis", 
            model=model_path, 
            device=0 if torch.cuda.is_available() else -1
        )
        
        # æµ‹è¯•æ–‡æœ¬ (åŒ…å«å¤šç§è¯­è¨€)
        test_texts = [
            "This is absolutely wonderful!",  # è‹±è¯­
            "Je suis trÃ¨s content avec ce produit.",  # æ³•è¯­
            "Ich bin sehr enttÃ¤uscht von dieser Erfahrung.",  # å¾·è¯­
            "Estoy bastante satisfecho con el servicio.",  # è¥¿ç­ç‰™è¯­
            "è¿™ä¸ªäº§å“çœŸçš„å¾ˆç³Ÿç³•",  # ä¸­æ–‡
        ]
        
        # è¿›è¡Œæƒ…æ„Ÿåˆ†æ
        results = sentiment_analyzer(test_texts)
        
        # æ‰“å°ç»“æœ
        for text, result in zip(test_texts, results):
            print(f"ğŸ“ æ–‡æœ¬: {text}")
            print(f"  æƒ…æ„Ÿæ ‡ç­¾: {result['label']}, ç½®ä¿¡åº¦: {result['score']:.4f}")
            print("-" * 50)
        
        # æ¸…ç†å†…å­˜
        del sentiment_analyzer
        torch.cuda.empty_cache()
        return True
    except Exception as e:
        print(f"âŒ æƒ…æ„Ÿåˆ†ææ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        return False

def test_blip_inference(model_path: Path) -> bool:
    """æµ‹è¯• BLIP æ¨¡å‹çš„å›¾åƒæè¿°åŠŸèƒ½ã€‚"""
    try:
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        processor = BlipProcessor.from_pretrained(model_path)
        # ä½¿ç”¨ device_map="auto" ä»¥ä¾¿è‡ªåŠ¨åˆ†é…è®¾å¤‡
        model = BlipForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # å‡†å¤‡ä¸€å¼ æµ‹è¯•å›¾ç‰‡ (ä»ç½‘ç»œåŠ è½½)
        url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        response = requests.get(url)
        raw_image = Image.open(BytesIO(response.content)).convert('RGB')

        # å‡†å¤‡è¾“å…¥
        # å°†è¾“å…¥ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
        inputs = processor(raw_image, return_tensors="pt").to(model.device, torch.float16)

        # ç”Ÿæˆæè¿°
        out = model.generate(**inputs, max_new_tokens=75)
        caption = processor.decode(out[0], skip_special_tokens=True)
        
        print(f"ğŸ–¼ï¸ BLIP æ¨ç†ç»“æœ: {caption}")
        # æ¸…ç†å†…å­˜
        del model, processor, inputs, out
        torch.cuda.empty_cache()
        return True
    except Exception as e:
        print(f"âŒ BLIP æ¨ç†å¤±è´¥: {e}")
        return False

def test_deepseek_inference(model_path: Path) -> bool:
    """æµ‹è¯•DeepSeekæ¨¡å‹çš„æ–‡æœ¬ç”ŸæˆåŠŸèƒ½ã€‚"""
    try:
        # ä½¿ç”¨ AutoTokenizer å’Œ AutoModelForCausalLM åŠ è½½æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_path, device_map= "auto")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,  # bfloat16 å¯¹æ–°ä¸€ä»£GPUæ›´å‹å¥½
            device_map="auto",
        )
        # å‡†å¤‡è¾“å…¥
        messages = [
            {"role": "user", "content": "è¯·å†™ä¸€ä¸ªå…³äºæœˆäº®å’Œæ˜Ÿæ˜Ÿçš„ç«¥è¯æ•…äº‹ã€‚"}
        ]
        # ä½¿ç”¨ apply_chat_template æ˜¯å¤„ç†å¯¹è¯æ¨¡å‹çš„æœ€ä½³å®è·µ
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        # ç”Ÿæˆæ–‡æœ¬
        outputs = model.generate(
            **inputs,
            max_new_tokens= 16384,
            do_sample= True,
            top_p= 0.9,
            temperature= 0.6,
            repetition_penalty=1.1
        )
        # è§£ç å¹¶æ‰“å°ç»“æœ
        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        print(f"ğŸ¤– DeepSeek æ¨ç†ç»“æœ: {response}")
        return True
    except Exception as e:
        print(f"âŒ DeepSeek æ¨ç†å¤±è´¥: {e}")
        return False

def test_whisper_inference(model_path):
    try:
        processor = WhisperProcessor.from_pretrained(model_path, device_map= "auto")
        model = WhisperForConditionalGeneration.from_pretrained(model_path, device_map= "auto")
        model.generation_config.forced_decoder_ids = None

        ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
        sample = ds[0]["audio"]

        input_features = processor(
            sample["array"],
            sampling_rate=sample["sampling_rate"],
            return_tensors="pt"
        ).input_features
        input_features = input_features.to("cuda")
        predicted_ids = model.generate(input_features)
        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        print("ğŸ”Š Whisper è¯†åˆ«ç»“æœï¼š", transcription)
        return True
    except Exception as e:
        print("âŒ Whisper æ¨ç†å¤±è´¥ï¼š", e)
        return False

def test_qwen3_inference(model_path, cache_dir= "/mnt/7T/xz/"):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, device_map= "auto")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            cache_dir= cache_dir,
            device_map= "auto"
        )

        messages = [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"},
        ]
        conversation = ""
        for m in messages:
            conversation += f"{m['role']}: {m['content']}\n"

        input_ids = tokenizer.encode(conversation + tokenizer.eos_token, return_tensors="pt").to("cuda")
        output = model.generate(input_ids, pad_token_id= tokenizer.eos_token_id)
        response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)
        print("ğŸ§  Qwen3 æ¨ç†ç»“æœï¼š", response)

        del model
        del tokenizer
        return True
    except Exception as e:
        print("âŒ Qwen3 æ¨ç†å¤±è´¥ï¼š", e)
        return False

def test_qwen_vl_inference(model_path):
    try:
        from qwen_vl_utils import process_vision_info
        from transformers import Qwen2_5_VLForConditionalGeneration  # ç¡®ä¿è·¯å¾„æ­£ç¡®

        processor = AutoProcessor.from_pretrained(model_path, device_map= "auto")
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map="auto"
        )

        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                    },
                    {"type": "text", "text": "è¯·æè¿°è¿™å¼ å›¾ç‰‡"},
                ],
            }
        ]

        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to("cuda")

        output_ids = model.generate(**inputs, max_new_tokens=64)
        response = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
        print("ğŸ–¼ï¸ Qwen-VL å›¾æ–‡æ¨ç†ç»“æœï¼š", response)
        return True
    except Exception as e:
        print("âŒ Qwen-VL æ¨ç†å¤±è´¥ï¼š", e)
        return False

def test_t5_inference(model_path: Path) -> bool:
    """æµ‹è¯• T5 æ¨¡å‹çš„æ–‡æœ¬æ‘˜è¦åŠŸèƒ½ã€‚"""
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # å‡†å¤‡æµ‹è¯•æ–‡æœ¬
        text = """The Tower of London is a historic castle on the north bank of 
                the River Thames in central London. It was founded towards the 
                end of 1066 as part of the Norman Conquest. The Tower has served 
                variously as an armory, a treasury, a menagerie, the home of the 
                Royal Mint, a public records office, and the home of the Crown 
                Jewels of England."""
        
        # ç”Ÿæˆè¾“å…¥
        inputs = tokenizer(
            "summarize: " + text,  # T5 éœ€è¦ä»»åŠ¡å‰ç¼€
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(model.device)
        
        # ç”Ÿæˆæ‘˜è¦
        outputs = model.generate(
            **inputs,
            max_length=150,
            min_length=40,
            num_beams=4,
            early_stopping=True
        )
        
        # è§£ç ç»“æœ
        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ“ T5 æ‘˜è¦ç»“æœ: {summary}")
        
        # æ¸…ç†å†…å­˜
        del model, tokenizer, inputs, outputs
        torch.cuda.empty_cache()
        return True
    except Exception as e:
        print(f"âŒ T5 æ¨ç†å¤±è´¥: {e}")
        return False

def download_model(model_name: str, local_model_folder: str= "./"):
    if model_name == "Qwen/Qwen3-32B":
        local_path = os.path.join(local_model_folder, "Qwen/Qwen3-32B")
        if Path(local_path).exists() and test_qwen3_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ Qwen3 æ¨¡å‹ï¼š{local_path}")
            return

        tokenizer = AutoTokenizer.from_pretrained(model_name, device_map= "auto")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map= "auto"
        )
        tokenizer.save_pretrained(local_path)
        model.save_pretrained(local_path)
        print(f"âœ… Qwen3 æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{local_path}")
        test_qwen3_inference(local_path)
    
    elif model_name == "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":
        local_path = os.path.join(local_model_folder, "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")
        if Path(local_path).exists() and test_deepseek_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ DeepSeek r1 æ¨¡å‹ï¼š{local_path}")
            return

        tokenizer = AutoTokenizer.from_pretrained(model_name, device_map= "auto")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map= "auto"
        )
        tokenizer.save_pretrained(local_path)
        model.save_pretrained(local_path)
        print(f"âœ… DeepSeek r1 æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{local_path}")
        test_deepseek_inference(local_path)

    elif model_name == "Qwen/Qwen2.5-VL-32B-Instruct":
        local_path = os.path.join(local_model_folder, "Qwen/Qwen2.5-VL-32B-Instruct")
        if Path(local_path).exists() and test_qwen_vl_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ Qwen-VL æ¨¡å‹ï¼š{local_path}")
            return
        processor = AutoProcessor.from_pretrained(model_name, device_map= "auto")
        model = AutoModelForImageTextToText.from_pretrained(model_name, device_map= "auto")
        processor.save_pretrained(local_path)
        model.save_pretrained(local_path)
        processor.save_pretrained(local_path)
        print(f"âœ… Qwen-VL æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{local_path}")
        test_qwen_vl_inference(local_path)

    elif model_name == "openai/whisper-medium":
        local_path = os.path.join(local_model_folder, "whisper_models/whisper-medium")        
        if Path(local_path).exists() and test_whisper_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ Whisper æ¨¡å‹ï¼š{local_path}")
            return

        processor = WhisperProcessor.from_pretrained(model_name, device_map= "auto")
        model = WhisperForConditionalGeneration.from_pretrained(model_name, device_map= "auto")
        processor.save_pretrained(local_path)
        model.save_pretrained(local_path)
        print(f"âœ… Whisper æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{local_path}")
        test_whisper_inference(local_path)

    elif model_name == "Salesforce/blip-image-captioning-large":
        # å®šä¹‰ä¸€ä¸ªæ›´ç®€æ´çš„æœ¬åœ°æ–‡ä»¶å¤¹å
        local_path = os.path.join(local_model_folder, "blip-image-captioning-large")
        if Path(local_path).exists() and test_blip_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ BLIP æ¨¡å‹ï¼š{local_path}")
            return

        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ BLIP æ¨¡å‹: {model_name}...")
        processor = BlipProcessor.from_pretrained(
            model_name,
            force_download=True,  # å¼ºåˆ¶é‡æ–°ä¸‹è½½
            resume_download=False,  # ä¸æ¢å¤ä¸‹è½½
            local_files_only=False,  # ä¸ä½¿ç”¨æœ¬åœ°ç¼“å­˜
            use_fast=False,  # æ˜ç¡®æŒ‡å®šä¸ä½¿ç”¨ fast tokenizer
        )
        model = BlipForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto" # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        )
        
        # åˆ›å»ºç›®å½•å¹¶ä¿å­˜
        Path(local_path).mkdir(parents=True, exist_ok=True)
        processor.save_pretrained(local_path)
        model.save_pretrained(local_path)
        
        print(f"âœ… BLIP æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{local_path}")
        # ä¸‹è½½åç«‹å³æµ‹è¯•
        test_blip_inference(local_path)

    elif model_name== "t5-base":
        model_name = "t5-base"
        languages= ["en"]
        local_path = os.path.join(local_model_folder, "t5-base")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ä¸”å¯ç”¨
        if Path(local_path).exists() and test_t5_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ T5-base æ¨¡å‹: {local_path}")
            return
        
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ T5-base æ¨¡å‹...")
        
        # ä¸‹è½½æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # ä¿å­˜åˆ°æœ¬åœ°
        Path(local_path).mkdir(parents=True, exist_ok=True)
        tokenizer.save_pretrained(local_path)
        model.save_pretrained(local_path)
        
        print(f"âœ… T5-base æ¨¡å‹å·²ä¿å­˜åˆ°: {local_path}")
        # æµ‹è¯•æ¨¡å‹
        test_t5_inference(local_path)
    elif model_name== "easyocr":
        model_name = "easyocr"
        languages= ["en"]
        local_path = os.path.join(local_model_folder, "easyocr")
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ä¸”å¯ç”¨
        if Path(local_path).exists() and test_easyocr_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ easyocr æ¨¡å‹: {local_path}")
            return
        
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ easyocr æ¨¡å‹...")

        # æµ‹è¯•æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ä¸”å¯ç”¨
        if Path(local_path).exists():
            print("ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œæ­£åœ¨éªŒè¯...")
            if test_easyocr_inference(local_path):
                print(f"âœ… ç°æœ‰æ¨¡å‹éªŒè¯é€šè¿‡ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
                return 

        # ä¸‹è½½æ¨¡å‹
        print("â³ æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
        start_time = time.time()
        
        # æ­¤è°ƒç”¨ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
        reader = easyocr.Reader(
            lang_list= languages,
            gpu=True,
            download_enabled=True,
            model_storage_directory=local_path,
            detector=True,
            recognizer=True
        )
        
        download_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œè€—æ—¶ {download_time:.2f}s")
        
        # ç«‹å³æµ‹è¯•
        print("\nğŸ§ª å¼€å§‹æ¨¡å‹æµ‹è¯•...")
        test_result = test_easyocr_inference(local_path, languages)
        # æ¸…ç†èµ„æº
        del reader
        torch.cuda.empty_cache()        
        if not test_result:
            raise RuntimeError("æ¨¡å‹æµ‹è¯•å¤±è´¥")
    elif model_name== "nlptown/bert-base-multilingual-uncased-sentiment":
        local_path = os.path.join(local_model_folder, model_name)
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ä¸”å¯ç”¨
        if Path(local_path).exists() and test_sentiment_analysis_inference(local_path):
            print(f"âœ… å·²å­˜åœ¨å¯ç”¨ {model_name} æ¨¡å‹: {local_path}")
            return
        
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ easyocr æ¨¡å‹...")
        sentiment_analyzer = pipeline(
            "sentiment-analysis", 
            model=model_name,
            device="cuda"
        )
        
        # ä¿å­˜åˆ°æœ¬åœ°
        Path(local_path).mkdir(parents=True, exist_ok=True)
        sentiment_analyzer.model.save_pretrained(local_path)
        sentiment_analyzer.tokenizer.save_pretrained(local_path)
        
        print(f"âœ… å¤šè¯­è¨€æƒ…æ„Ÿåˆ†ææ¨¡å‹å·²ä¿å­˜åˆ°: {local_path}")

        # æµ‹è¯•æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ä¸”å¯ç”¨
        if Path(local_path).exists():
            print("ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œæ­£åœ¨éªŒè¯...")
            if test_sentiment_analysis_inference(local_path):
                print(f"âœ… ç°æœ‰æ¨¡å‹éªŒè¯é€šè¿‡ï¼Œæ— éœ€é‡æ–°ä¸‹è½½")
                return 
    else:
        print(f"âŒ æœªçŸ¥æ¨¡å‹åç§°ï¼š{model_name}")


if __name__ == "__main__":
    # å¯æ ¹æ®éœ€è¦ä¿®æ”¹è¿™é‡Œ
    # download_model("Qwen/Qwen3-32B")
    # download_model("Qwen/Qwen2.5-VL-32B-Instruct")
    # download_model("deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")
    # download_model("openai/whisper-medium")
    # download_model("Salesforce/blip-image-captioning-large")
    # download_model("t5-base")
    # download_model("easyocr")
    download_model("nlptown/bert-base-multilingual-uncased-sentiment")