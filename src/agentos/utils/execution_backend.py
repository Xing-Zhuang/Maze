import ray
import os
import subprocess
import time
import requests
import json
import signal
from abc import ABC, abstractmethod
from typing import Optional, Dict, List, Tuple

# ==============================================================================
# 1. æŠ½è±¡åŸºç±» (The "Contract")
# ==============================================================================
class ExecutionBackend(ABC):
    """
    ä¸€ä¸ªæŠ½è±¡åŸºç±»ï¼Œå®šä¹‰äº†æ‰€æœ‰æ¨¡å‹æ‰§è¡Œåç«¯å¿…é¡»å®ç°çš„æ¥å£ã€‚
    """
    
    @abstractmethod
    def get_backend_type(self) -> str:
        """è¿”å›åç«¯çš„ç±»å‹å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ 'vllm' æˆ– 'huggingface'ã€‚"""
        pass

    @abstractmethod
    def deploy(self, node_id: str, gpu_index: int, model_name: str) -> Optional[Dict]:
        """
        ä¸ºæ¨¡å‹å‡†å¤‡æ‰§è¡Œç¯å¢ƒã€‚
        æˆåŠŸåˆ™è¿”å›ä¸€ä¸ªåŒ…å«è°ƒåº¦æ‰€éœ€ä¿¡æ¯ï¼ˆå¦‚api_urlï¼‰çš„å­—å…¸ï¼Œå¤±è´¥åˆ™è¿”å›Noneã€‚
        """
        pass

    @abstractmethod
    def undeploy(self, node_id: str, gpu_index: int) -> bool:
        """æ¸…ç†æ‰§è¡Œç¯å¢ƒã€‚"""
        pass

# ==============================================================================
# 2. vLLM åç«¯å®ç° (VLLMBackend and its Helper)
# ==============================================================================

@ray.remote
class VLLMRunner:
    """
    åœ¨è¿œç¨‹èŠ‚ç‚¹ä¸Šå®é™…æ‰§è¡ŒvLLMå¯åœæ“ä½œçš„Ray Actorã€‚
    """
    def __init__(self, proj_path: str, model_folder: str, model_name: str, gpu_indices: List[int], vllm_params: Dict):
        self.proj_path = proj_path
        self.model_path = os.path.join(proj_path, model_folder, model_name)
        self.gpu_indices = gpu_indices
        self.vllm_params = vllm_params
        self.process = None
        self.port = self._find_free_port()
        self.node_ip = ray.util.get_node_ip_address()
        self.api_url = f"http://{self.node_ip}:{self.port}"
        # --- æ–°å¢ï¼šåˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶å¥æŸ„ ---
        self.stdout_log = None
        self.stderr_log = None

    def _find_free_port(self):
        """åœ¨èŠ‚ç‚¹ä¸Šæ‰¾åˆ°ä¸€ä¸ªéšæœºçš„ç©ºé—²ç«¯å£ã€‚"""
        import socket
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))
            return s.getsockname()[1]

    def start_server(self) -> None:
        """
        å¯åŠ¨vLLMæœåŠ¡å™¨è¿›ç¨‹ï¼Œå¹¶å°†å…¶æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºé‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶ã€‚
        """
        import torch
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA is not available!")
        print(f"CUDA devices: {torch.cuda.device_count()}")

        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, self.gpu_indices))
        print(f"  -> Starting vLLM server and model_path: {self.model_path} on GPUs {self.gpu_indices} at port {self.port}...")
        vllm_command = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", self.model_path,
            "--host", "0.0.0.0",
            "--port", str(self.port),
        ]
        
        for key, value in self.vllm_params.items():
            vllm_command.append(f"--{key}")
            if value is not None:
                 vllm_command.append(str(value))
        vllm_command.append("--trust-remote-code")
        try:
            model_display_name = self.vllm_params.get("served-model-name", os.path.basename(self.model_path))
            print(f"ğŸš€ Launching vLLM for model '{model_display_name}' on GPUs {self.gpu_indices} at port {self.port}...")
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šé‡å®šå‘vLLMè¿›ç¨‹çš„è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶ ---
            log_dir = os.path.join(self.proj_path, "src", "agentos", "logs", "vllm_logs")
            os.makedirs(log_dir, exist_ok=True)
            print(f"  -> vllm log_dir: {log_dir}")
            model_name_safe = model_display_name.replace("/", "_") # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
            
            stdout_log_path = os.path.join(log_dir, f"{model_name_safe}_{self.port}_stdout.log")
            stderr_log_path = os.path.join(log_dir, f"{model_name_safe}_{self.port}_stderr.log")

            print(f"  -> ğŸ“œ vLLM stdout log: {stdout_log_path}")
            print(f"  -> ğŸ“œ vLLM stderr log: {stderr_log_path}")

            self.stdout_log = open(stdout_log_path, 'w')
            self.stderr_log = open(stderr_log_path, 'w')
            
            self.process = subprocess.Popen(
                vllm_command, 
                env=env, 
                stdout=self.stdout_log, 
                stderr=self.stderr_log,
                preexec_fn= os.setsid
            )
            # --- ä¿®æ”¹ç»“æŸ ---

        except Exception as e:
            print(f"âŒ vLLM process Popen failed for model '{model_display_name}': {e}")
            if self.process: self.process.kill()

    def stop_server(self):
        if self.process:
            print(f"ğŸ›‘ Stopping vLLM server process group at port {self.port}...")
            try:
                # --- æ ¸å¿ƒä¿®æ”¹2ï¼šç»ˆæ­¢æ•´ä¸ªè¿›ç¨‹ç»„ ---
                # è·å–è¿›ç¨‹ç»„ID (pgid)ï¼Œå¹¶å‘æ•´ä¸ªç»„å‘é€ç»ˆæ­¢ä¿¡å·
                pgid = os.getpgid(self.process.pid)
                os.killpg(pgid, signal.SIGTERM) # ä½¿ç”¨SIGTERMè¿›è¡Œæ›´ä¼˜é›…çš„å…³é—­
                
                # ç­‰å¾…è¿›ç¨‹ç»“æŸ
                self.process.wait(timeout=5)
                print(f"  -> Process group for port {self.port} terminated.")
            except (ProcessLookupError, TimeoutError) as e:
                 print(f"  -> Graceful shutdown failed ({e}), attempting forceful kill...")
                 try:
                     # å¦‚æœä¼˜é›…å…³é—­å¤±è´¥ï¼Œåˆ™ä½¿ç”¨SIGKILLå¼ºåˆ¶æ€æ­»
                     os.killpg(os.getpgid(self.process.pid), signal.SIGKILL)
                 except Exception as kill_e:
                     print(f"  -> Forceful kill failed: {kill_e}")
            
            self.process = None
        
        if self.stdout_log: self.stdout_log.close()
        if self.stderr_log: self.stderr_log.close()
        return True

    def get_api_url(self) -> str:
        return self.api_url

class VLLMBackend(ExecutionBackend):
    """
    vLLMæ‰§è¡Œåç«¯çš„å…·ä½“å®ç°ã€‚
    ç®¡ç†vLLMå®ä¾‹çš„éƒ¨ç½²ã€å¸è½½å’ŒçŠ¶æ€ã€‚
    """
    def __init__(self, resource_manager, proj_path, model_folder, models_config_path):
        self.resource_manager = resource_manager 
        self.proj_path= proj_path
        self.model_folder = model_folder
        self.active_runners = {} # æ ¼å¼: {(node_id, gpu_index): actor_handle}
        try:
            with open(models_config_path, 'r') as f:
                self.model_config = json.load(f)
            print("âœ… VLLMBackend: Model configuration loaded from 'models.json'.")
        except Exception as e:
            self.model_config = {}
            print("ğŸ˜­ VLLMBackend: Model configuration loaded from 'models.json'.")

    def get_backend_type(self) -> str:
        return "vllm"

    def deploy(self, node_id: str, gpu_indices: List[int], model_name: str):
        if not gpu_indices: return None
        key = (node_id, frozenset(gpu_indices))
        if key in self.active_runners: return
        runner_name = f"vllm_runner_{node_id}_{'_'.join(map(str, gpu_indices))}"
        print(f"  -> Assigning Actor name: {runner_name}")
        model_info = self.model_config.get(model_name)
        if not model_info: return
        INTERNAL_SCHEDULER_KEYS = ['tensor-parallel-size', 'max-model-len']
        vllm_params = {k: v for k, v in model_info.items() if k in INTERNAL_SCHEDULER_KEYS}
        vllm_params["served-model-name"] = model_name

        try:
            runner_actor = VLLMRunner.options(
                name= runner_name,
                num_gpus= len(gpu_indices),
                scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(node_id, soft=False)
            ).remote(self.proj_path, self.model_folder, self.model_config.get(model_name).get("path"), gpu_indices, vllm_params)
            runner_actor.start_server.remote()
            self.active_runners[key] = runner_actor
            print(f"â³ VLLMBackend: Deployment initiated for '{model_name}' on Node {node_id[:6]}/GPUs {gpu_indices}.")
        
        except Exception as e:
            print(f"âŒ VLLMBackend deployment failed to initiate: {e}")


    def undeploy(self, node_id: str, gpu_indices: List[int]) -> bool:
        if not gpu_indices: return False
        key = (node_id, frozenset(gpu_indices))
        runner_actor = self.active_runners.pop(key, None)
        
        if runner_actor:
            try:
                print(f"  -> ğŸ“© Sending stop signal to VLLMRunner on node {node_id}...")
                # å°è¯•ä¼˜é›…åœ°åœæ­¢æœåŠ¡ï¼Œä½†è®¾ç½®ä¸€ä¸ªè¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
                ray.get(runner_actor.stop_server.remote(), timeout=15) 
                print(f"  -> âœ… VLLMRunner on node {node_id} stopped gracefully.")
            except ray.exceptions.ActorDiedError:
                # å¦‚æœ Actor åœ¨æˆ‘ä»¬å°è¯•åœæ­¢å®ƒæ—¶æ­»äº¡ï¼ˆå¾ˆå¯èƒ½æ˜¯å› ä¸ºèŠ‚ç‚¹è¢«ç»ˆæ­¢ï¼‰ï¼Œ
                # æˆ‘ä»¬å¯ä»¥è®¤ä¸ºå¸è½½ç›®æ ‡å·²è¾¾æˆï¼Œå› ä¸º Actor ç¡®å®æ¶ˆå¤±äº†ã€‚
                print(f"  -> âš ï¸  VLLMRunner actor died during stop command (as expected if node is terminating). Continuing cleanup.")
            except ray.exceptions.ActorUnavailableError as e:
                # Actor å·²ç»ä¸å¯ç”¨ï¼ˆå¦‚ SIGTERMï¼‰ï¼Œé€šå¸¸æ˜¯èŠ‚ç‚¹æˆ–è¿›ç¨‹è¢«å¤–éƒ¨ç»ˆæ­¢
                print(f"  -> âš ï¸  VLLMRunner actor unavailable (likely already terminated): {e}. Continuing cleanup.")
            except ray.exceptions.RayTaskError as e:
                # æ•è· Actor å†…éƒ¨åœ¨æ‰§è¡Œ stop_server æ—¶å‘ç”Ÿçš„é”™è¯¯
                print(f"  -> âŒ Error occurred inside the VLLMRunner's stop_server method: {e}")
            except Exception as e:
                # æ•è·å…¶ä»–å¯èƒ½çš„å¼‚å¸¸ï¼Œä¾‹å¦‚è¶…æ—¶
                print(f"  -> âŒ An unexpected error occurred during undeploy: {type(e).__name__} - {e}")
            finally:
                # æ— è®ºä¸Šé¢å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæœ€åéƒ½è¦å°è¯• kill Actor å¥æŸ„ã€‚
                # è¿™èƒ½ç¡®ä¿ Ray æ¸…ç†å…¶å†…éƒ¨çŠ¶æ€ï¼Œé‡Šæ”¾èµ„æºã€‚
                print(f"  -> ğŸ’¥ Killing actor handle to ensure resource release in Ray.")
                ray.kill(runner_actor)

            self.resource_manager.update_gpu_state(node_id, gpu_indices, {
                "status": "FREE",
                "model_name": None, 
                "backend_type": None, 
                "request_api_url": None,
                "runner_key": None,
                "deployment_finish_time": None})
            print(f"  -> ğŸŸ¢ Resources for node {node_id}, GPUs {gpu_indices} have been marked as FREE.")
            return True
        return False

    def is_server_ready(self, node_id: str, gpu_indices: List[int]) -> Tuple[bool, Optional[str]]:
        """
        é€šè¿‡HTTPè¯·æ±‚æ£€æŸ¥vLLMæœåŠ¡æ˜¯å¦å·²åŠ è½½æ¨¡å‹å¹¶å‡†å¤‡å°±ç»ªã€‚
        """
        # ã€ä¿®æ”¹ã€‘keyçš„ç”Ÿæˆæ–¹å¼ä»¥æ”¯æŒå¤šå¡
        key = (node_id, frozenset(gpu_indices))
        runner_actor = self.active_runners.get(key)
        if not runner_actor:
            return False, None
        try:
            api_url = ray.get(runner_actor.get_api_url.remote())
            health_check_url = f"{api_url}/health"
            response = requests.get(health_check_url, timeout=3)
            # --- æ ¸å¿ƒä¿®å¤ï¼šåªæ£€æŸ¥çŠ¶æ€ç ï¼Œä¸è§£æJSON ---
            # .raise_for_status() ä¼šåœ¨çŠ¶æ€ç ä¸æ˜¯2xxï¼ˆå¦‚ 404, 500ï¼‰æ—¶è‡ªåŠ¨æŠ›å‡ºå¼‚å¸¸
            response.raise_for_status() 
            # å¦‚æœä»£ç èƒ½æ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜çŠ¶æ€ç æ˜¯2xxï¼Œå¥åº·æ£€æŸ¥æˆåŠŸ
            print(f"  -> âœ… Health check successful for {api_url}. Server is ready.")
            return True, api_url
            # --- ä¿®å¤ç»“æŸ ---
        except requests.exceptions.RequestException as e:
            # åªæœ‰åœ¨è¯·æ±‚å¤±è´¥ï¼ˆå¦‚è¿æ¥è¶…æ—¶ã€DNSé”™è¯¯ã€é2xxçŠ¶æ€ç ï¼‰æ—¶æ‰ä¼šè¿›å…¥è¿™é‡Œ
            if 'api_url' in locals():
                 print(f"   -> [Debug] Health check for {api_url} failed: {type(e).__name__} - {e}")
            return False, None

# ==============================================================================
# 3. HuggingFace Transformers åç«¯å®ç°
# ==============================================================================

class HuggingFaceBackend(ExecutionBackend):
    """
    ä»£è¡¨ç›´æ¥ä½¿ç”¨HuggingFace Transformersåº“åœ¨æœ¬åœ°åŠ è½½æ¨¡å‹çš„æ–¹å¼ã€‚
    """
    def get_backend_type(self) -> str:
        return "huggingface"

    def deploy(self, node_id: str, gpu_index: int, model_name: str) -> Optional[Dict]:
        # å¯¹äºæœ¬åœ°åŠ è½½ï¼Œæ— éœ€éƒ¨ç½²æœåŠ¡ï¼Œåªéœ€æ›´æ–°èµ„æºç®¡ç†å™¨çš„çŠ¶æ€å³å¯
        print(f"âœ… Preparing to run '{model_name}' with HuggingFaceBackend on Node {node_id[:6]}, GPU {gpu_index}.")
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åŒæ ·å°†ä¼šå ç”¨GPUï¼Œæ‰€ä»¥ä¹Ÿéœ€è¦æ›´æ–°èµ„æºçŠ¶æ€
        # self.resource_manager.update_gpu_state(...) # åœ¨TaskSchedulerä¸­å¤„ç†èµ„æºå ç”¨
        return {"backend_type": "huggingface"}

    def undeploy(self, node_id: str, gpu_index: int) -> bool:
        # æœ¬åœ°åŠ è½½çš„æ¨¡å‹åœ¨ä»»åŠ¡ç»“æŸåç”±remote_task_runnerè‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜ï¼Œæ— éœ€é¢å¤–æ“ä½œ
        print(f"âœ… HuggingFaceBackend task finished on Node {node_id[:6]}, GPU {gpu_index}. Resources are auto-released.")
        return True