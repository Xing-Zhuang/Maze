import os
import sys
import json
import time
import threading
import traceback
import networkx as nx
import queue
from typing import Dict
import uuid
import logging

def dag_manager_daps(args, dag_que: queue.Queue, completion_que: queue.Queue, task_scheduler, shutdown_event, w1=2.0, w2=1.0):
    """
    (V3 - å†…å­˜é€šä¿¡ç‰ˆ)
    - ä»å†…å­˜é˜Ÿåˆ—æ¥æ”¶å·¥ä½œæµã€‚
    - é€šè¿‡ç›´æ¥æ–¹æ³•è°ƒç”¨æäº¤ä»»åŠ¡ã€‚
    - ä»å†…å­˜é˜Ÿåˆ—æ¥æ”¶ä»»åŠ¡å®Œæˆé€šçŸ¥ã€‚
    """
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸŒŸ FTC Scheduler (V3 - In-Memory Version) starting...")

    dags: Dict[str, nx.DiGraph] = {}
    dags_lock = threading.Lock()
    submit_queue = queue.PriorityQueue()
    task_enqueue_index = 0
    task_enqueue_lock = threading.Lock()

    dag_type_avg_times = {} 
    task_type_avg_times = {'io': 2.0, 'cpu': 5.0, 'gpu': 30.0}
    max_known_inferences = 10.0
    logger = logging.getLogger(__name__)

    def enqueue_task(dag, run_id, node_id, is_start_node=False):
        """å°†å°±ç»ªä»»åŠ¡æ”¾å…¥ä¼˜å…ˆçº§é˜Ÿåˆ—ã€‚"""
        scheduler_start_time = time.time()
        nonlocal task_enqueue_index
        node_info = dag.nodes[node_id]
        task_type = node_info.get('resources', {}).get('type', 'cpu')

        # æš‚æ—¶ä½¿ç”¨é»˜è®¤å¹³å‡æ—¶é—´è¿›è¡Œé¢„æµ‹
        pred_time = task_type_avg_times.get(task_type, 5.0)
        pred_exec_time = pred_time
        pred_cost_time = 0.0
        
        node_info['pred_time'] = pred_time
        
        remaining_inf = float(dag.graph.get('remaining_inferences', 0))
        score_urgency = 1.0 - (remaining_inf / max_known_inferences) if max_known_inferences > 0 else 0

        dag_type = dag.graph['dag_type']
        expected_dag_time = dag_type_avg_times.get(dag_type, 300.0)
        score_criticality = pred_time / expected_dag_time if expected_dag_time > 0 else 0

        performance_score = w1 * score_urgency + w2 * score_criticality
        
        with task_enqueue_lock:
            current_index = task_enqueue_index
            task_enqueue_index += 1
            
        sub_time = dag.graph.get("sub_time", time.time())
        priority_tuple = (sub_time, -performance_score, current_index)
        scheduler_end_time= time.time()
        item_to_queue = (priority_tuple, run_id, node_id, scheduler_end_time - scheduler_start_time, pred_exec_time, pred_cost_time)
        submit_queue.put(item_to_queue)
        logger.info(f"  -> Enqueued '{node_info.get('name', node_id)}'. (Arrival: {priority_tuple[0]:.2f}, PerfScore: {-priority_tuple[1]:.2f})")

    def dag_creator():
        """ä»æäº¤é˜Ÿåˆ—ä¸­è·å–æ•°æ®åŒ…ï¼Œåœ¨å†…å­˜ä¸­é‡å»ºå›¾ï¼Œå¹¶é¢„æ³¨å†Œæ‰€æœ‰ä»»åŠ¡ã€‚"""
        nonlocal max_known_inferences
        while not shutdown_event.is_set():
            try:
                submission_package = dag_que.get()
                
                if not isinstance(submission_package, dict) or submission_package.get("submission_type") != "dynamic_agent":
                    print(f"âš ï¸ [dag_creator] æ”¶åˆ°æœªçŸ¥æ ¼å¼çš„æ•°æ®ï¼Œå·²è·³è¿‡: {submission_package}")
                    continue

                run_id = submission_package['run_id']
                server_root_path = submission_package['server_root_path']
                workflow_payload = submission_package['workflow_payload']
                
                print(f"ğŸ˜Š [dag_creator] å¼€å§‹å¤„ç†æ–°çš„åŠ¨æ€Agentå·¥ä½œæµ, run_id: {run_id[:8]}")

                dag = nx.node_link_graph(workflow_payload['graph_definition'])
                
                tasks_definition = workflow_payload['tasks']
                for task_id, task_payload_dict in tasks_definition.items():
                    if task_id in dag.nodes:
                        dag.nodes[task_id].update(task_payload_dict)

                dag.graph['run_id'] = run_id
                dag.graph['dag_id'] = workflow_payload.get('dag_id', str(uuid.uuid4()))
                dag.graph['server_root_path'] = server_root_path
                dag.graph['name'] = workflow_payload['name']
                dag.graph['arrival_time'] = time.time() 
                dag.graph['sub_time'] = time.time()
                dag.graph['dag_type'] = workflow_payload['name']
                dag.graph['lock'] = threading.Lock()
                
                total_inferences = sum(1 for node_id in dag.nodes if dag.nodes[node_id].get('resources', {}).get('type') == 'gpu')
                dag.graph['total_inferences'] = total_inferences
                dag.graph['remaining_inferences'] = total_inferences
                max_known_inferences = max(max_known_inferences, float(total_inferences))
                
                with dags_lock:
                    dags[run_id] = dag
                
                print(f"  - âœ… [dag_creator] å†…å­˜å›¾æ„å»ºå®Œæˆ for run_id: {run_id[:8]}.")

                # --- æ–°å¢é€»è¾‘å¼€å§‹ ---
                print(f"  - â³ [dag_creator] é¢„æ³¨å†Œå·¥ä½œæµä¸­çš„æ‰€æœ‰ {len(dag.nodes)} ä¸ªä»»åŠ¡...")
                for task_id, node_data_view in dag.nodes(data=True):
                    # å¤åˆ¶ä¸€ä»½èŠ‚ç‚¹æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹å›¾æ•°æ®
                    node_data = dict(node_data_view)
                    
                    # æ„å»º TaskStatusManager.add_task éœ€è¦çš„ task_info å­—å…¸
                    task_info = {
                        "run_id": run_id, 
                        "dag_id": dag.graph.get("dag_id", ""),
                        "task_id": task_id,
                        "func_name": node_data.get("name"),
                        "serialized_func": node_data.get('serialized_func'),
                        "inputs": node_data.get("inputs", {}),
                        "output_parameters": node_data.get("meta", {}).get("output_parameters", {}),
                        # ä» 'resources' å­—å…¸ä¸­å®‰å…¨åœ°è·å–èµ„æºä¿¡æ¯
                        "type": node_data.get("resources", {}).get("type", "cpu"),
                        "cpu_num": node_data.get("resources", {}).get("cpu_num", 1),
                        "mem": node_data.get("resources", {}).get("mem", 1024),
                        "gpu_mem": node_data.get("resources", {}).get("gpu_mem", 0),
                        "server_root_path": server_root_path,
                        "arrival_time": dag.graph['arrival_time']
                    }
                    task_scheduler.status_mgr.add_task(task_info)
                print(f"  - âœ… [dag_creator] æ‰€æœ‰ä»»åŠ¡é¢„æ³¨å†Œå®Œæˆã€‚")
                # --- æ–°å¢é€»è¾‘ç»“æŸ ---


                for node_id, in_degree in dag.in_degree():
                    if in_degree == 0:
                        dag.nodes[node_id]["in_degree"] = -1
                        enqueue_task(dag, run_id, node_id, is_start_node=True)
                    else:
                        dag.nodes[node_id]["in_degree"] = in_degree
            
            except Exception as e:
                print(f"âŒ [Error] In dag_creator: {e}\n{traceback.format_exc()}")
            time.sleep(0.01)

    def scheduler_and_submitter():
        """é€šè¿‡ç›´æ¥è°ƒç”¨ TaskScheduler.submit() æ–¹æ³•æäº¤ä»»åŠ¡ã€‚"""
        logger.info("ğŸš€ FTC Submitter (In-Memory Version) is running.")
        
        while not shutdown_event.is_set():
            try:
                priority_tuple, run_id, task_id, scheduler_cost_time, pred_exec_time, pred_cost_time = submit_queue.get()
                with dags_lock:
                    dag = dags.get(run_id)
                if not dag:
                    logger.warning(f"  -> â“ [Submitter] Warning: DAG for run_id {run_id} not found. Task '{task_id}' skipped.")
                    continue
                
                node_data = dict(dag.nodes[task_id])
                logger.info(f"ğŸ† Picked '{node_data.get('name', task_id)}' for submission.")
                dag.nodes[task_id]['scheduler_cost_time'] = scheduler_cost_time
                dag.nodes[task_id]['pred_exec_time'] = pred_exec_time
                dag.nodes[task_id]['pred_cost_time'] = pred_cost_time

                serialized_func = node_data.get('serialized_func')
                if not serialized_func:
                    raise ValueError(f"Serialized function not found in graph for task '{task_id}'")
                server_root_path = dag.graph.get("server_root_path")
                payload = {
                    "priority": priority_tuple,
                    "run_id": run_id, 
                    "dag_id": dag.graph.get("dag_id", ""),
                    "task_id": task_id,
                    "func_name": node_data.get("name"),
                    "serialized_func": serialized_func,
                    "inputs": node_data.get("inputs", {}),
                    "output_parameters": node_data.get("meta", {}).get("output_parameters", {}),
                    "type": node_data.get("resources", {}).get("type", "cpu"),
                    "cpu_num": node_data.get("resources", {}).get("cpu_num", 1),
                    "mem": node_data.get("resources", {}).get("mem", 1024),
                    "gpu_mem": node_data.get("resources", {}).get("gpu_mem", 0),
                    "server_root_path": server_root_path,
                }
                task_scheduler.submit(payload)
                logger.info(f"ğŸ Submitted: '{node_data.get('name')}' from Run ID {run_id[:8]}")

            except Exception as e:
                logger.error(f"âŒ [CRITICAL ERROR] In submitter loop: {e}\n{traceback.format_exc()}")
            time.sleep(0.05)

    def monitor():
        """ä»å†…å­˜é˜Ÿåˆ—è·å–ä»»åŠ¡å®Œæˆé€šçŸ¥ï¼Œå¹¶è®°å½•æ—¶é—´ã€‚"""
        print(f"ğŸ’  FTC Monitor (In-Memory Version) is listening...")
        
        while not shutdown_event.is_set():
            try:
                notification = completion_que.get()
                
                run_id = notification.get("run_id")
                task_id = notification.get("task_id")
                status = notification.get("status")

                # --- æ–°å¢ï¼šè®°å½•ä»»åŠ¡æ—¶é—´ ---
                if status in ["finished", "failed"]:
                    start_time = notification.get("worker_start_exec_time")
                    end_time = notification.get("worker_end_time")
                    if start_time and end_time:
                        timing_data = {
                            "start_time": start_time,
                            "end_time": end_time,
                        }
                        task_scheduler.status_mgr.record_task_completion(run_id, task_id, timing_data)
                # --- æ–°å¢ç»“æŸ ---

                with dags_lock:
                    dag = dags.get(run_id)
                if not dag: continue
                
                node_info = dag.nodes.get(task_id, {})
                func_name = node_info.get("name", "unknown_task")
                
                print(f"âœ… Monitor: Received '{status}' for '{func_name}' (DAG: {dag.graph.get('name', 'N/A')[:8]})")
                
                with dag.graph['lock']:
                    if status == "finished":
                        if node_info.get('resources', {}).get('type') == 'gpu':
                            dag.graph['remaining_inferences'] = max(0, dag.graph.get('remaining_inferences', 1) - 1)
                        
                        node_info.update(notification)
                    
                    is_dag_complete = True
                    successors = list(dag.successors(task_id)) if dag.has_node(task_id) else []
                    for successor_id in successors:
                        is_dag_complete = False
                        successor_node = dag.nodes[successor_id]
                        if successor_node.get("in_degree", 0) > 0:
                            successor_node["in_degree"] -= 1
                            if successor_node["in_degree"] == 0:
                                successor_node["in_degree"] = -1
                                enqueue_task(dag, run_id, successor_id)
                    
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†ï¼ˆåŒ…æ‹¬é‚£äº›æ²¡æœ‰åç»§çš„ä»»åŠ¡ï¼‰
                    all_nodes_finished = all(d.get('in_degree', -1) == -1 for n, d in dag.nodes.items())

                    if is_dag_complete and all_nodes_finished:
                        dag_total_time = time.time() - dag.graph['arrival_time']
                        # --- æ–°å¢ï¼šè®°å½•å·¥ä½œæµæ€»æ—¶é—´ ---
                        task_scheduler.status_mgr.record_run_completion(run_id, dag_total_time)
                        
                        print(f"ğŸ‰ DAG '{dag.graph.get('name', '')}' COMPLETE in {dag_total_time:.2f}s.")
                        with dags_lock:
                            if run_id in dags:
                                del dags[run_id]

            except Exception as e:
                print(f"âŒ [Error] In monitor loop: {e}\n{traceback.format_exc()}")
            time.sleep(0.01)

    # --- å¯åŠ¨æ‰€æœ‰çº¿ç¨‹ ---
    threads = [
        threading.Thread(target=dag_creator, daemon=True, name="DAGCreator"),
        threading.Thread(target=scheduler_and_submitter, daemon=True, name="Submitter"),
        threading.Thread(target=monitor, daemon=True, name="Monitor")
    ]
    
    for t in threads:
        t.start()

    logger.info("ğŸŒŸ DAPS Scheduler threads started.")
    shutdown_event.wait()
    logger.info("ğŸŒŸ DAPS Scheduler has received shutdown signal and is exiting.")