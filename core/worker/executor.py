import os
import ray
import time
import queue
import torch
import random
import threading
import hashlib  # æ–°å¢
import zipfile  # æ–°å¢
import io       # æ–°å¢
import cloudpickle
from typing import Optional, Dict, List, Tuple, Any
import tracemalloc
import requests
from maze.utils.execution_backend import VLLMBackend, HuggingFaceBackend
import heapq
import gc
import re
from collections import deque, defaultdict
import json
import inspect
from maze.agent.config import config
from maze.utils.log_config import setup_logging # <--- 1. å¯¼å…¥æˆ‘ä»¬çš„æ–°å‡½æ•°
import logging # <--- å¯¼å…¥logging

def _calculate_local_sha256(file_path):
    """ä¸€ä¸ªè®¡ç®—æ–‡ä»¶SHA256å“ˆå¸Œå€¼çš„è¾…åŠ©å‡½æ•°"""
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        while True:
            data = f.read(65536)
            if not data:
                break
            sha256.update(data)
    return sha256.hexdigest()

def write_log(dag_id,run_id,task_id,dag_func_file,func_name,status):
    log_dir = './log'
    log_file = os.path.join(log_dir, 'log.txt')
    os.makedirs(log_dir, exist_ok=True)
    with open(log_file, 'a', encoding='utf-8') as file:
        file.write(f"{dag_id},{run_id},{task_id},{dag_func_file},{func_name},{status},{time.time()}\n")

@ray.remote(num_cpus=0, max_calls=1)
def remote_task_runner(
    serialized_func: bytes,
    task_id: str,
    run_id: str,
    master_addr: str,
    task_inputs: Dict,
    output_parameters: Dict,
    task_type:str,
    gpu_indices: Optional[List[int]],
    ctx_actor: object
) -> Dict[str, Any]:
    """
    (V8 - æœ€ç»ˆæ— Redisç‰ˆ)
    - ç›´æ¥ä»å‚æ•°æ¥æ”¶å‡½æ•°å­—èŠ‚ç ã€‚
    - ç›´æ¥è¿”å›å…ƒæ•°æ®å­—å…¸ï¼Œä¸å†å†™å…¥Redisã€‚
    """
    setup_logging()
    logger = logging.getLogger(__name__)
    worker_start_exec_time = time.time()
    try:
        # --- é˜¶æ®µ1ï¼šæ–‡ä»¶åŒæ­¥æ‹‰å– (Pull) ---
        logger.debug(f"ğŸ”„ [Sync] Task {task_id} on worker starting file synchronization for run '{run_id}'...")
        original_dir = os.getcwd()
        data_dir = os.path.join(original_dir, "taskspace")
        os.makedirs(data_dir, exist_ok=True)

        master_hashes_url = f"http://{master_addr}/files/hashes/{run_id}"
        
        response = requests.get(master_hashes_url)
        response.raise_for_status()
        master_hashes = response.json().get('hashes', {})
        logger.debug(f"  - [Sync] Fetched {len(master_hashes)} official file hashes from master.")
        
        local_hashes = {}
        for root, _, files in os.walk(data_dir):
            for name in files:
                file_abs_path = os.path.join(root, name)
                file_rel_path = os.path.relpath(file_abs_path, data_dir)
                local_hashes[file_rel_path] = _calculate_local_sha256(file_abs_path)

        files_to_update = [
            rel_path for rel_path, master_hash in master_hashes.items()
            if rel_path not in local_hashes or local_hashes[rel_path] != master_hash
        ]
        
        if files_to_update:
            logger.debug(f"  - [Sync] Found {len(files_to_update)} files to update: {files_to_update}")
            master_download_url = f"http://{master_addr}/files/download/{run_id}"
            response = requests.post(master_download_url, json={"files": files_to_update})
            response.raise_for_status()
            
            with zipfile.ZipFile(io.BytesIO(response.content)) as zf:
                zf.extractall(data_dir)
            logger.debug(f"  - [Sync] Worker directory updated successfully.")
        else:
            logger.debug("  - [Sync] All local files are up-to-date.")

        # --- é˜¶æ®µ2ï¼šå‚æ•°è§£æ ---
        logger.debug(f"ğŸ” [Resolve] Task {task_id} starting robust parameter resolution...")
        func = cloudpickle.loads(serialized_func)

        if task_type == "gpu" and gpu_indices:
            visible_devices = ",".join(map(str, gpu_indices))
            logger.debug(f"  -> [EnvSetup] Setting CUDA_VISIBLE_DEVICES='{visible_devices}' for task {task_id}.")
            os.environ["CUDA_VISIBLE_DEVICES"] = visible_devices

        def _resolve_single_source_remote(source: str, context_actor: object) -> Any:
            index = None
            base_source = source
            index_match = re.search(r'\[(\d+)\]$', source)
            if index_match:
                index = int(index_match.group(1))
                base_source = source[:index_match.start()]
            
            parts = base_source.split('.')
            if len(parts) < 3:
                raise ValueError(f"æ— æ•ˆçš„è¾“å…¥æºæ ¼å¼: '{source}'ã€‚å¿…é¡»ä½¿ç”¨ 'task_id.output.key' çš„æ ¼å¼ã€‚")
                
            upstream_task_id, _, output_key = parts[0], parts[1], ".".join(parts[2:])
            
            # --- æ ¸å¿ƒä¿®æ­£ï¼šæ¨¡æ‹Ÿå•æœºç‰ˆçš„ä¸¤æ­¥æŸ¥æ‰¾ ---
            # 1. å…ˆä» DAGContext è·å–ä¸Šæ¸¸ä»»åŠ¡è¿”å›çš„ã€æ•´ä¸ªç»“æœå¯¹è±¡ã€‘
            upstream_result_obj = ray.get(context_actor.get.remote(f"{upstream_task_id}.output"))
            
            # 2. ç„¶åå†ä»è¿™ä¸ªå¯¹è±¡ä¸­è·å–å…·ä½“çš„å­—æ®µ
            if not isinstance(upstream_result_obj, dict):
                 raise TypeError(f"ä»»åŠ¡ '{upstream_task_id}' çš„è¾“å‡ºä¸æ˜¯å­—å…¸ï¼Œæ— æ³•è®¿é—®ã€‚")
            if output_key not in upstream_result_obj:
                raise KeyError(f"åœ¨ä»»åŠ¡ '{upstream_task_id}' çš„è¾“å‡ºä¸­æ‰¾ä¸åˆ°é”®: '{output_key}'ã€‚")
            
            resolved_object = upstream_result_obj[output_key]
            # --- ä¿®æ­£ç»“æŸ ---

            if index is not None:
                if isinstance(resolved_object, (list, tuple)):
                    if index >= len(resolved_object):
                        raise IndexError(f"ç´¢å¼• {index} è¶…å‡ºèŒƒå›´ã€‚æº: '{source}' çš„åˆ—è¡¨é•¿åº¦ä¸º {len(resolved_object)}ã€‚")
                    return resolved_object[index]
                else:
                    raise TypeError(f"å°è¯•å¯¹éåºåˆ—ç±»å‹è¿›è¡Œç´¢å¼•è®¿é—®ã€‚æº: '{source}'")
            return resolved_object

        resolved_kwargs = {}
        # éå†å‡½æ•°ç­¾åä¸­çš„æ¯ä¸€ä¸ªå‚æ•°
        for param in inspect.signature(func).parameters.values():
            param_name = param.name
            # ä¼˜å…ˆçº§ 1: æ£€æŸ¥ç”¨æˆ·åœ¨ add_task æ—¶æä¾›çš„ 'task_inputs' å­—å…¸
            if param_name in task_inputs:
                source = task_inputs[param_name]
                if isinstance(source, list):
                    resolved_kwargs[param_name] = [
                        _resolve_single_source_remote(item, ctx_actor) if isinstance(item, str) and ".output" in item else item
                        for item in source
                    ]
                elif isinstance(source, str) and ".output" in source:
                    resolved_kwargs[param_name] = _resolve_single_source_remote(source, ctx_actor)
                else:
                    resolved_kwargs[param_name] = source
            # å¦‚æœåœ¨ inputs ä¸­æ²¡æ‰¾åˆ°ï¼Œåˆ™è¿›å…¥ ä¼˜å…ˆçº§2 å’Œ 3
            else:
                found_in_config = False    
                # ä¼˜å…ˆçº§ 2: ä» DAGContext ä¸­æŒ‰é”®ååŒ¹é… (Contextä¸­å·²åŒ…å«pathså’Œonline_apis)
                try:
                    # ç›´æ¥å°è¯•ä» context actor è·å–åŒåå‚æ•°
                    value = ray.get(ctx_actor.get.remote(param_name))
                    resolved_kwargs[param_name] = value                    
                    found_in_config = True
                except KeyError:
                    pass
                # ä¼˜å…ˆçº§ 3: å¦‚æœé…ç½®ä¸­ä¹Ÿæ²¡æ‰¾åˆ°ï¼Œåˆ™ä½¿ç”¨å‡½æ•°å®šä¹‰çš„é»˜è®¤å€¼
                if not found_in_config and param.default is not inspect.Parameter.empty:
                    resolved_kwargs[param_name] = param.default
        logger.debug("  - âœ… [Resolve] All parameters resolved successfully.")
        
        # --- é˜¶æ®µ3ï¼šæ‰§è¡Œç”¨æˆ·å‡½æ•° ---
        try:
            # å…³é”®ï¼šåˆ‡æ¢åˆ°æ•°æ®ç›®å½•æ‰§è¡Œç”¨æˆ·å‡½æ•°
            os.chdir(data_dir)
            logger.debug(f"ğŸš€ [Execute] Task {task_id} starting execution in isolated data directory '{data_dir}'...")
            user_function_result = func(**resolved_kwargs)
            logger.debug(f"  - âœ… [Execute] Task {task_id} finished execution.")
        finally:
            # å…³é”®ï¼šæ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼Œéƒ½åˆ‡å›åŸå§‹ç›®å½•
            os.chdir(original_dir)

        # --- æ ¸å¿ƒä¿®æ­£ï¼šå®ç°ä¸ _run_local å®Œå…¨ä¸€è‡´çš„æ™ºèƒ½åŒ…è£…é€»è¾‘ ---
        final_output_obj = None
        if user_function_result is not None:
            output_dict = {}
            # ä»ä¼ é€’è¿‡æ¥çš„å…ƒæ•°æ®ä¸­è§£æè¾“å‡ºå­—æ®µå
            keys = list(output_parameters.get('properties', {}).keys())
            
            # ä¸¥æ ¼éµå®ˆâ€œå¿…é¡»ä¸”åªèƒ½æœ‰ä¸€ä¸ªè¾“å‡ºâ€çš„è§„åˆ™
            if len(keys) != 1:
                raise ValueError(
                    f"Task '{func.__name__}'s @tool decorator in output_parameters "
                    f"must define exactly one output property, but {len(keys)} were found: {keys}"
                )
            
            output_key = keys[0]
            output_dict[output_key] = user_function_result
            final_output_obj = output_dict
        
        # å°†ã€åŒ…è£…åçš„ã€‘ç»“æœå¯¹è±¡ï¼ˆæˆ– Noneï¼‰å­˜å…¥ DAGContext
        ray.get(ctx_actor.put.remote(f"{task_id}.output", final_output_obj))
        
        # --- é˜¶æ®µ4ï¼šæ–‡ä»¶åŒæ­¥æ¨é€ (Push) ---
        logger.debug(f"â¤´ï¸ [Push] Task {task_id} checking for file changes to push...")
        files_to_push = {}
        for root, dirs, files in os.walk(data_dir):
            if '__pycache__' in dirs:
                dirs.remove('__pycache__')
            if '.git' in dirs:
                dirs.remove('.git')

            for name in files:
                if name.endswith('.pyc') or name.endswith('.py'):
                    continue
                
                file_abs_path = os.path.join(root, name)
                file_rel_path = os.path.relpath(file_abs_path, data_dir)
                
                current_hash = _calculate_local_sha256(file_abs_path)
                
                if file_rel_path not in master_hashes or master_hashes[file_rel_path] != current_hash:
                    with open(file_abs_path, 'rb') as f:
                        files_to_push[file_rel_path] = f.read()

        if files_to_push:
            logger.debug(f"  - [Push] Found {len(files_to_push)} new/modified DATA files to upload.")
            upload_url = f"http://{master_addr}/files/upload/{run_id}"
            multipart_files = [(path, (path, content)) for path, content in files_to_push.items()]
            response = requests.post(upload_url, files=multipart_files)
            response.raise_for_status()
            logger.debug("  - âœ… [Push] Files uploaded to master successfully.")
        else:
            logger.debug("  - [Push] No data file changes detected.")

        return {
            "status": "finished", 
            "worker_start_exec_time": worker_start_exec_time,
            "end_time": time.time()}
    except Exception as e:
        import traceback
        error_message = f"Error in remote_task_runner: {e}\n{traceback.format_exc()}"
        logger.debug(f"[FAILED] Task {task_id} failed with error: {error_message}")
        return {"status": "failed", "err_msg": error_message}